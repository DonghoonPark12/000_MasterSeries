{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLIP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "참조 자료\n",
    "- https://www.youtube.com/watch?v=HkkaKI6NN-8\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이하 아래 코드에서 파 볼만한 것들  \n",
    "- clip.load(\"ViT-B/32\", device=device) <- done\n",
    "- clip.tokenize <- done\n",
    "- model.encode_image(image)\n",
    "- model.encode_text(text)\n",
    "- model(image, text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img.png](https://github.com/openai/CLIP/raw/main/CLIP.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 예시1\n",
    "- 다이어그램 그림을 다이어그램으로 인식함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RN50', 'RN101', 'RN50x4', 'RN50x16', 'RN50x64', 'ViT-B/32', 'ViT-B/16', 'ViT-L/14', 'ViT-L/14@336px']\n",
      "Model parameters: 151,277,313\n",
      "Input resolution: 224\n",
      "Context length: 77\n",
      "Vocab size: 49408\n",
      "\n",
      "\n",
      "Label probs: [[0.9927   0.004253 0.003016]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"D:/00_PILSA/CLIP\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(clip.available_models())\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "model.cuda().eval()\n",
    "input_resolution = model.visual.input_resolution\n",
    "context_length = model.context_length\n",
    "vocab_size = model.vocab_size\n",
    "\n",
    "# Vit-B/32 가장 작은 모델도 파라미터 수가 1.5억개 이다.\n",
    "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
    "print(\"Input resolution:\", input_resolution)\n",
    "print(\"Context length:\", context_length)\n",
    "print(\"Vocab size:\", vocab_size)\n",
    "\n",
    "image = preprocess(Image.open(\"D:/00_PILSA/CLIP/CLIP.png\")).unsqueeze(0).to(device) # torch.Size([1, 3, 224, 224])\n",
    "text = clip.tokenize([\"a diagram\", \"a dog\", \"a cat\"]).to(device) # torch.Size([3, 77])\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "with torch.no_grad():\n",
    "    image_features = model.encode_image(image) # torch.Size([1, 512])\n",
    "    text_features = model.encode_text(text) # torch.Size([3, 512])\n",
    "    \n",
    "    logits_per_image, logits_per_text = model(image, text)\n",
    "    probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
    "\n",
    "print(\"Label probs:\", probs)  # prints: [[0.9927937  0.00421068 0.00299572]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 예시2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- clip을 zero-shot prediction으로 활용하기 위해서는 아래 그림 처럼 가능한 클래스를 모두 token으로 만든다.\n",
    "- 즉, \"a photo of a beaver\" ~ \"a photo of a tractor\" 까지 100개의 클래스를 모두 [1, 512] 사이즈로 임베딩 해야 하니 [100, 512] 가 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "\n",
      "Top predictions:\n",
      "\n",
      "        oak_tree: 40.16%\n",
      "     willow_tree: 25.93%\n",
      "      maple_tree: 15.97%\n",
      "       pine_tree: 14.10%\n",
      "       palm_tree: 1.19%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2716f8e3610>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import clip\n",
    "import torch\n",
    "from torchvision.datasets import CIFAR100\n",
    "\n",
    "# Load the model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load('ViT-B/32', device)\n",
    "\n",
    "# Download the dataset\n",
    "cifar100 = CIFAR100(root=os.path.expanduser(\"~/.cache\"), download=True, train=False)\n",
    "\n",
    "# Prepare the inputs\n",
    "image, class_id = cifar100[3635]\n",
    "image_input = preprocess(image).unsqueeze(0).to(device)\n",
    "# cifar 클래스의 \"a photo of a {object}\" 는 일단 다 생성한다!!\n",
    "text_inputs = torch.cat([clip.tokenize(f\"a photo of a {c}\") for c in cifar100.classes]).to(device)\n",
    "\n",
    "# Calculate features\n",
    "with torch.no_grad():\n",
    "    image_features = model.encode_image(image_input)\n",
    "    text_features = model.encode_text(text_inputs)\n",
    "\n",
    "# Pick the top 5 most similar labels for the image\n",
    "image_features /= image_features.norm(dim=-1, keepdim=True) # torch.Size([1, 512])\n",
    "text_features /= text_features.norm(dim=-1, keepdim=True) # torch.Size([100, 512]) \n",
    "# 두개의 임베딩을 dot product 한다!!\n",
    "simirality = (100.0 * image_features @ text_features.T).softmax(dim=-1) # torch.Size([1, 100])\n",
    "values, indices = simirality[0].topk(5) # 텐서는 기본적으로 topk() 메소드가 내장되어 있나 보다\n",
    "\n",
    "# Print the result\n",
    "print(\"\\nTop predictions:\\n\")\n",
    "for value, index in zip(values, indices):\n",
    "    print(f\"{cifar100.classes[index]:>16s}: {100 * value.item():.2f}%\")\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "plt.imshow(image, interpolation='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 이하 CLIP 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(texts: Union[str, List[str]], context_length: int=77, truncate: bool=False) -> Union[torch.IntTensor, torch.LongTensor]:\n",
    "    \"\"\" 주어진 입력 스트링에 대해 토큰화된 표현으로 변경\n",
    "\n",
    "        context_length : int\n",
    "            문맥 길이, 모든 CLIP 모델은 77을 문맥 길이(?)로 사용한다.\n",
    "\n",
    "        truncate: bool\n",
    "            인코딩된 것이 문맥 길이 보다 길 경우 자른 건지 여부\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        shape = [입력 스트링 길이, 문맥 길이]의 2차원 텐서 반환\n",
    "    \"\"\"\n",
    "    if isinstance(texts, str):\n",
    "            texts = [texts]\n",
    "\n",
    "        sot_token = _tokenizer.encoder[\"<|startoftext|>\"]\n",
    "        eot_token = _tokenizer.encoder[\"<|endoftext|>\"]\n",
    "        all_tokens = [[sot_token] + _tokenizer.encode(text) + [eot_token] for text in texts]\n",
    "\n",
    "        # 2차원 빈 텐서 생성\n",
    "        if packaging.version.parse(torch.__version__) < packaging.version.parse(\"1.8.0\"):\n",
    "            result = torch.zeros(len(all_tokens), context_length, dtype=torch.long)\n",
    "        else:\n",
    "            result = torch.zeros(len(all_tokens), context_length, dtype=torch.int)\n",
    "\n",
    "        for i, tokens in enumerate(all_tokens):\n",
    "            if len(tokens) > context_length:\n",
    "                if truncate:\n",
    "                    tokens = tokens[:context_length]\n",
    "                    tokens[-1] = eot_token\n",
    "                else:\n",
    "                    raise RuntimeError(f\"Input {texts[i]} is too long for context length {context_length}\")\n",
    "            result[i, :len(tokens)] = torch.tensor(tokens)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIP(nn.Module):\n",
    "    def __init__(self,\n",
    "                 embed_dim: int,\n",
    "                 # vision\n",
    "                 image_resolution: int,\n",
    "                 vision_layers: Union[Tuple[int, int, int, int], int],\n",
    "                 vision_width: int,\n",
    "                 vision_patch_size: int,\n",
    "                 # text\n",
    "                 context_length: int,\n",
    "                 vocab_size: int,\n",
    "                 transformer_width: int,\n",
    "                 transformer_heads: int,\n",
    "                 transformer_layers: int\n",
    "                 ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.context_length = context_length\n",
    "\n",
    "        if isinstance(vison_layers, (tuple, list)):\n",
    "            vision_heads = vision_width * 32 // 64\n",
    "            self.visual = ModifiedResNet(\n",
    "                layers = vision_layers,\n",
    "                output_dim = embed_dim,\n",
    "                heads = vision_heads,\n",
    "                input_resolution = image_resolution,\n",
    "                width = vision_width\n",
    "            )\n",
    "        else:\n",
    "            vision_heads = vision_width // 64\n",
    "            self.visual = VisionTransformer(\n",
    "                layers=vision_layers,\n",
    "                output_dim=embed_dim,\n",
    "                heads=vision_heads,\n",
    "                input_resolution=image_resolution,\n",
    "                width=vision_width,\n",
    "                patch_size=vision_patch_size\n",
    "            )\n",
    "        \n",
    "        self.transformer = Transformer(\n",
    "            width=transformer_width,\n",
    "            layers=transformer_layers,\n",
    "            heads=transformer_heads,\n",
    "            attn_mask=self.build_attention_mask()\n",
    "        )\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.token_embedding = nn.Embedding(vocab_size, transformer_width) # nn.Embedding과 nn.Parameter 차이는??\n",
    "        self.positional_embedding = nn.Parameter(torch.empty(self.context_length, transformer_width))\n",
    "        self.ln_final = LayerNorm(transformer_width)\n",
    "\n",
    "        self.text_projection = nn.Parameter(torch.empty(transformer_width, embed_dim))\n",
    "        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\n",
    "\n",
    "        self.initialize_parameters()\n",
    "\n",
    "    def initialize_parameters(self):\n",
    "        nn.init.normal_(self.token_embedding.weight, std=0.02)\n",
    "        nn.init.normal_(self.positional_embedding, std=0.01)\n",
    "\n",
    "        if isinstance(self.visual, ModifiedResNet):\n",
    "            if self.visual.attnpool is not None:\n",
    "                std = self.visual.attnpool.c_proj.in_features ** -0.5 # c_proj?? -sqrt()??\n",
    "                nn.init.normal_(self.visual.attnpool.q_proj.weight, std=std)\n",
    "                nn.init.normal_(self.visual.attnpool.k_proj.weight, std=std)\n",
    "                nn.init.normal_(self.visual.attnpool.v_proj.weight, std=std)\n",
    "                nn.init.normal_(self.visual.attnpool.c_proj.weight, std=std)\n",
    "\n",
    "            for resnet_block in [self.visual.layer1, self.visual.layer2, self.visual.layer3, self.visual.layer4]:\n",
    "                for name, param in resnet_block.named_parameters():\n",
    "                    if name.endswith(\"bn3.weight\"):\n",
    "                        nn.init.zeros_(param)\n",
    "\n",
    "        proj_std = (self.transformer.width ** -0.5) * ((2 * self.transformer.layers) ** -0.5)\n",
    "        attn_std = self.transformer.width ** -0.5\n",
    "        fc_std = (2 * self.transformer.width) ** -0.5\n",
    "        for block in self.transformer.resblocks:\n",
    "            nn.init.normal_(block.attn.in_proj_weight, std=attn_std)\n",
    "            nn.init.normal_(block.attn.out_proj.weight, std=proj_std)\n",
    "            nn.init.normal_(block.mlp.c_fc.weight, std=fc_std)\n",
    "            nn.init.normal_(block.mlp.c_proj.weight, std=proj_std)\n",
    "        \n",
    "        if self.text_projection is not None:\n",
    "            nn.init.normal_(self.text_projection, std=self.transformer.width ** -0.5)\n",
    "\n",
    "    def build_attention_mask(self):\n",
    "        # lazily create causal attention mask, with full attention between the vision tokens\n",
    "        # pytorch uses additive attention mask; fill with -inf\n",
    "        mask = torch.empty(self.context_length, self.context_length)\n",
    "        mask.fill_(float(\"-inf\"))\n",
    "        mask.triu_(1)  # zero out the lower diagonal\n",
    "        return mask"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.17 ('torchWin_rev')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dcfd0bbc3e25ae79d1f134c08b87e3777d49169aec2e55e1b5fb8249822c085d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
