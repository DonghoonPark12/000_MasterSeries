{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAIN WITH CUSTOMIZED DATASETS\n",
    "아래 예시는 Waymo 데이터 셋을 이용해 3가지 단계를 진행한다.  \n",
    "1. 커스텀 데이터 셋 준비\n",
    "2. Config 준비\n",
    "3. 커스텀 데이터로 학습, 테스트, 추론"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 커스텀 데이터셋 준비\n",
    "MMDetection3D에서는 새로운 데이터셋을 지원하는 3가지 방법이 존재한다.\n",
    "1. 데이터 셋을 존재하는 format으로 변경\n",
    "2. 데이터 셋을 표준 format으로 변경\n",
    "3. 새로운 데이터셋을 구현  \n",
    "일반적으로 위의 두 방법을 추천한다.  \n",
    "\n",
    "  \n",
    "※ 본 글에서는 Waymo 형식을 Kitti 포맷으로 변환하는 것을 예로 든다. 다른 예로 Lyft를 nuScenes로 변환하는 것은 새로운 데이터 컨버터를 구현하는 것이, 다른 데이터 포맷으로 변환하는 것 대비 쉽다(?)  \n",
    "--> 표준 fotmat이 무엇을 말하는지는 뒤에 살펴본다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1-1. KITTI dataset format 으로 변환하기\n",
    "- Imageset은 training/validation/testing 으로 나눌 데이터 목록 txt 파일을 포함한다.\n",
    "- calib은 켈리브레이션 정보, image_2, velodyne, label_2는 각각 이미지, 라이다, 라벨 정보 포함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmdetection3d\n",
    "├── mmdet3d\n",
    "├── tools\n",
    "├── configs\n",
    "├── data\n",
    "│   ├── kitti\n",
    "│   │   ├── ImageSets\n",
    "│   │   ├── testing\n",
    "│   │   │   ├── calib\n",
    "│   │   │   ├── image_2\n",
    "│   │   │   ├── velodyne\n",
    "│   │   ├── training\n",
    "│   │   │   ├── calib\n",
    "│   │   │   ├── image_2\n",
    "│   │   │   ├── label_2\n",
    "│   │   │   ├── velodyne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Values    Name      Description\n",
    "----------------------------------------------------------------------------\n",
    "   1    type         Describes the type of object: 'Car', 'Van', 'Truck',\n",
    "                     'Pedestrian', 'Person_sitting', 'Cyclist', 'Tram',\n",
    "                     'Misc' or 'DontCare'\n",
    "   1    truncated    Float from 0 (non-truncated) to 1 (truncated), where\n",
    "                     truncated refers to the object leaving image boundaries     : 이미지 경계를 벗어낫는지 유무\n",
    "   1    occluded     Integer (0,1,2,3) indicating occlusion state:\n",
    "                     0 = fully visible, 1 = partly occluded\n",
    "                     2 = largely occluded, 3 = unknown\n",
    "   1    alpha        Observation angle of object, ranging [-pi..pi]              : 관찰 각도\n",
    "   4    bbox         2D bounding box of object in the image (0-based index):\n",
    "                     contains left, top, right, bottom pixel coordinates\n",
    "   3    dimensions   3D object dimensions: height, width, length (in meters)\n",
    "   3    location     3D object location x,y,z in camera coordinates (in meters)   : 카메라 좌표계 기준 물체 위치\n",
    "   1    rotation_y   Rotation ry around Y-axis in camera coordinates [-pi..pi]\n",
    "   1    score        Only for results: Float, indicating confidence in\n",
    "                     detection, needed for p/r curves, higher is better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Waymo 데이터셋 다운 후에 입력 데이터와 라벨을 KITTI 형식으로 바꿔야 한다.\n",
    "- 다음 KittiDataset을 상속한 WaymoDataset을 구현하여 학습하고, KittiMetric을 상속한 WaymoMetric을 구현하여 평가 가능하다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 특별히 mm3d에서는, Waymo를 KITTI 포맷으로 변환하는 컨버터를 구현해 놓았다.\n",
    "- 한편, Waymo는 자체 평가 방식이 있으므로, Waymo Metric을 추가로 구현해야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1-2. standard format dataset 으로 변환하기\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Waymo Dataset(CAM-Lidar)\n",
    "- 10Hz 즉, 360도 스캔에 0.1sec가 걸린다. Lidar, 카메라 등 모든 데이터 수집 빈도가 동일하다. [issue](https://github.com/waymo-research/waymo-open-dataset/issues/102)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Waymo Converter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/open-mmlab/mmdetection3d/blob/dev-1.x/tools/dataset_converters/waymo_converter.py\n",
    "\n",
    "# Copyright (c) OpenMMLab. All rights reserved.\n",
    "r\"\"\"Adapted from `Waymo to KITTI converter\n",
    "    <https://github.com/caizhongang/waymo_kitti_converter>`_.\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    from waymo_open_dataset import dataset_pb2\n",
    "except ImportError:\n",
    "    raise ImportError('Please run \"pip install waymo-open-dataset-tf-2-6-0\" '\n",
    "                      '>1.4.5 to install the official devkit first.')\n",
    "\n",
    "import os\n",
    "from glob import glob\n",
    "from os.path import exists, join\n",
    "\n",
    "import mmengine\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from waymo_open_dataset.utils import range_image_utils, transform_utils\n",
    "from waymo_open_dataset.utils.frame_utils import \\\n",
    "    parse_range_image_and_camera_projection\n",
    "\n",
    "\n",
    "class Waymo2KITTI(object):\n",
    "    \"\"\"Waymo to KITTI converter.\n",
    "\n",
    "    This class serves as the converter to change the waymo raw data to KITTI\n",
    "    format.\n",
    "\n",
    "    Args:\n",
    "        load_dir (str): Directory to load waymo raw data.\n",
    "        save_dir (str): Directory to save data in KITTI format.\n",
    "        prefix (str): Prefix of filename. In general, 0 for training, 1 for\n",
    "            validation and 2 for testing.\n",
    "        workers (int, optional): Number of workers for the parallel process.\n",
    "            Defaults to 64.\n",
    "        test_mode (bool, optional): Whether in the test_mode.\n",
    "            Defaults to False.\n",
    "        save_cam_sync_labels (bool, optional): Whether to save cam sync labels. 카메라 싱크 라벨??\n",
    "            Defaults to True.\n",
    "    \"\"\"    \n",
    "    def __init__(self,\n",
    "                 load_dir,\n",
    "                 save_dir, \n",
    "                 prefix,\n",
    "                 workers=64,\n",
    "                 test_mode=False,\n",
    "                 save_cam_sync_labels=True):\n",
    "        self.filter_empty_3dboxes = True\n",
    "        self.filter_no_label_zone_points = True\n",
    "\n",
    "        self.selected_waymo_classes = ['VEHICLE', 'PEDESTRIAN', 'CYCLIST']\n",
    "\n",
    "        # Only data collected in specific locations will be converted\n",
    "        # If set None, this filter is disabled\n",
    "        # Available options: location_sf (main dataset)\n",
    "        self.selected_waymo_locations = None # ??<------------------\n",
    "        self.save_track_id = False\n",
    "\n",
    "        # turn on eager execution for older tensorflow versions\n",
    "        if int(tf.__version__.split('.')[0]) < 2:\n",
    "            tf.enable_eager_execution()\n",
    "\n",
    "        # keep the order defined by the official protocol\n",
    "        self.cam_list = [\n",
    "            '_FRONT',\n",
    "            '_FRONT_LEFT',\n",
    "            '_FRONT_RIGHT',\n",
    "            '_SIDE_LEFT',\n",
    "            '_SIDE_RIGHT',\n",
    "        ]\n",
    "\n",
    "        self.lidar_list = ['TOP', 'FRONT', 'SIDE_LEFT', 'SIDE_RIGHT', 'REAR']\n",
    "        self.type_list = [\n",
    "            'UNKNOWN', 'VEHICLE', 'PEDESTRIAN', 'SIGN', 'CYCLIST'\n",
    "        ]\n",
    "        self.waymo_to_kitti_class_map = {\n",
    "            'UNKNOWN': 'DontCare',\n",
    "            'PEDESTRIAN': 'Pedestrian',\n",
    "            'VEHICLE': 'Car',\n",
    "            'CYCLIST': 'Cyclist',\n",
    "            'SIGN': 'Sign'  # not in kitti\n",
    "        }\n",
    "\n",
    "        self.load_dir = load_dir\n",
    "        self.save_dir = save_dir\n",
    "        self.prefix = prefix\n",
    "        self.workers = int(workers)\n",
    "        self.test_mode = test_mode\n",
    "        self.save_cam_sync_labels = save_cam_sync_labels\n",
    "\n",
    "        self.tfrecord_pathnames = sorted(\n",
    "            glob(join(self.load_dir, '*.tfrecord'))\n",
    "        )\n",
    "\n",
    "        self.label_save_dir = f'{self.save_dir}/label_'\n",
    "        self.label_all_save_dir = f'{self.save_dir}/label_all' # ??<------------------\n",
    "        self.image_save_dir = f'{self.save_dir}/image_'\n",
    "        self.calib_save_dir = f'{self.save_dir}/calib'\n",
    "        self.point_cloud_save_dir = f'{self.save_dir}/velodyne'\n",
    "        self.pose_save_dir = f'{self.save_dir}/pose'\n",
    "        self.timestamp_save_dir = f'{self.save_dir}/timestamp'\n",
    "        if self.save_cam_sync_labels:\n",
    "            self.cam_sync_label_save_dir = f'{self.save_dir}/cam_sync_label_'\n",
    "            self.cam_sync_label_all_save_dir = f'{self.save_dir}/cam_sync_label_all'\n",
    "        \n",
    "        self.create_folder()\n",
    "\n",
    "    def convert(self):\n",
    "        \"\"\"Convert action.\"\"\"\n",
    "        print('Start converting ...')\n",
    "        mmengine.track_parallel_progress(self.convert_one, range(len(self)), self.workers)\n",
    "        print('\\nFinished ...')\n",
    "\n",
    "    def convert_one(self, file_idx):\n",
    "        \"\"\"Convert action for single file.\n",
    "\n",
    "        Args:\n",
    "            file_idx (int): Index of the file to be converted.\n",
    "        \"\"\"\n",
    "        pathname = self.tfrecord_pathnames[file_idx]\n",
    "        dataset = tf.data.TFRecordDataset(pathname, compression_type='')\n",
    "\n",
    "        for frame_idx, data in enumerate(dataset):\n",
    "\n",
    "            frame = dataset_pb2.Frame()\n",
    "            frame.ParseFromString(bytearray(data.numpy()))\n",
    "            if (self.selected_waymo_locations is not None\n",
    "                    and frame.context.stats.location\n",
    "                    not in self.selected_waymo_locations):\n",
    "                continue\n",
    "\n",
    "            self.save_image(frame, file_idx, frame_idx)\n",
    "            self.save_calib(frame, file_idx, frame_idx)\n",
    "            self.save_lidar(frame, file_idx, frame_idx)\n",
    "            self.save_pose(frame, file_idx, frame_idx)\n",
    "            self.save_timestamp(frame, file_idx, frame_idx)\n",
    "\n",
    "            if not self.test_mode:\n",
    "                # TODO save the depth image for waymo challenge solution.\n",
    "                self.save_label(frame, file_idx, frame_idx)\n",
    "                if self.save_cam_sync_labels:\n",
    "                    self.save_label(frame, file_idx, frame_idx, cam_sync=True) # cam 동기화 라벨도 같이 저장??\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Length of the filename list.\"\"\"\n",
    "        return len(self.tfrecord_pathnames)\n",
    "\n",
    "    def save_image(self, frame, file_idx, frame_idx):\n",
    "        \"\"\"Parse and save the images in jpg format.\n",
    "\n",
    "        Args:\n",
    "            frame (:obj:`Frame`): Open dataset frame proto.\n",
    "            file_idx (int): Current file index.\n",
    "            frame_idx (int): Current frame index.\n",
    "        \"\"\"\n",
    "        for img in frame.images:\n",
    "            img_path = f'{self.image_save_dir}{str(img.name - 1)}/' + \\\n",
    "                f'{self.prefix}{str(file_idx).zfill(3)}' + \\\n",
    "                f'{str(frame_idx).zfill(3)}.jpg'\n",
    "            with open(img_path, 'wb') as fp:\n",
    "                fp.write(img.image)\n",
    "\n",
    "    def save_calib(self, frame, file_idx, frame_idx):\n",
    "        \"\"\"Parse and save the calibration data.\n",
    "\n",
    "        Args:\n",
    "            frame (:obj:`Frame`): Open dataset frame proto.\n",
    "            file_idx (int): Current file index.\n",
    "            frame_idx (int): Current frame index.\n",
    "        \"\"\"\n",
    "        # waymo front camera to kitti reference camera\n",
    "        T_front_cam_to_ref = np.array([[0.0, -1.0, 0.0], \n",
    "                                       [0.0, 0.0, -1.0],\n",
    "                                       [1.0, 0.0, 0.0]])\n",
    "        camera_calibs = []\n",
    "        R0_rect = [f'{i:e}' for i in np.eye(3).flatten()]\n",
    "        Tr_velo_to_cams = []\n",
    "        calib_context = ''\n",
    "\n",
    "        for camera in frame.context.camera_calibrations:\n",
    "            # extrinsic parameters\n",
    "            T_cam_to_vehicle = np.array(camera.extrinsic.transform).reshape(4, 4)\n",
    "            T_vehicle_to_cam = np.linalg.inv(T_cam_to_vehicle)\n",
    "            Tr_velo_to_cams = self.cart_to_homo(T_front_cam_to_ref) @ T_vehicle_to_cam\n",
    "            if camera.name == 1:  # FRONT = 1, see dataset.proto for details\n",
    "                self.T_velo_to_front_cam = Tr_velo_to_cam.copy()\n",
    "            Tr_velo_to_cam = Tr_velo_to_cam[:3, :].reshape((12, ))\n",
    "            Tr_velo_to_cams.append([f'{i:e}' for i in Tr_velo_to_cam])\n",
    "\n",
    "            # intrinsic parameters\n",
    "            camera_calib = np.zeros((3, 4))\n",
    "            camera_calib[0, 0] = camera.intrinsic[0]\n",
    "            camera_calib[1, 1] = camera.intrinsic[1]\n",
    "            camera_calib[0, 2] = camera.intrinsic[2]\n",
    "            camera_calib[1, 2] = camera.intrinsic[3]\n",
    "            camera_calib[2, 2] = 1\n",
    "            camera_calib = list(camera_calib.reshape(12))\n",
    "            camera_calib = [f'{i:e}' for i in camera_calib]\n",
    "            camera_calibs.append(camera_calib)\n",
    "\n",
    "        # all camera ids are saved as id-1 in the result because\n",
    "        # camera 0 is unknown in the proto\n",
    "        for i in range(5):\n",
    "            calib_context += 'P' + str(i) + ': ' + ' '.join(camera_calibs[i]) + '\\n'\n",
    "        \n",
    "        calib_context += 'R0_rect' + ': ' + ' '.join(R0_rect) + '\\n'\n",
    "        \n",
    "        for i in range(5):\n",
    "            calib_context += 'Tr_velo_to_cam_' + str(i) + ': ' + ' '.join(Tr_velo_to_cams[i]) + '\\n'\n",
    "        \n",
    "        with open(f'{self.calib_save_dir}/{self.prefix}' +\n",
    "                  f'{str(file_idx).zfill(3)}{str(frame_idx).zfill(3)}.txt',\n",
    "                  'w+') as fp_calib:\n",
    "            fp_calib.write(calib_context)\n",
    "            fp_calib.close()\n",
    "\n",
    "    def save_lidar(self, frame, file_idx, frame_idx):\n",
    "        \"\"\"Parse and save the lidar data in psd format.\n",
    "\n",
    "        Args:\n",
    "            frame (:obj:`Frame`): Open dataset frame proto.\n",
    "            file_idx (int): Current file index.\n",
    "            frame_idx (int): Current frame index.\n",
    "        \"\"\"\n",
    "        range_images, camera_projections, seg_labels, range_image_top_pose = \\\n",
    "            parse_range_image_and_camera_projection(frame)\n",
    "\n",
    "        if range_image_top_pose is None:\n",
    "            # the camera only split doesn't contain lidar points.\n",
    "            return\n",
    "        # First return\n",
    "        points_0, cp_points_0, intensity_0, elongation_0, mask_indices_0 = \\\n",
    "            self.convert_range_image_to_point_cloud(\n",
    "                frame,\n",
    "                range_images,\n",
    "                camera_projections,\n",
    "                range_image_top_pose,\n",
    "                ri_index=0\n",
    "            )\n",
    "        points_0 = np.concatenate(points_0, axis=0)\n",
    "        intensity_0 = np.concatenate(intensity_0, axis=0)\n",
    "        elongation_0 = np.concatenate(elongation_0, axis=0)\n",
    "        mask_indices_0 = np.concatenate(mask_indices_0, axis=0)\n",
    "\n",
    "        # Second return\n",
    "        points_1, cp_points_1, intensity_1, elongation_1, mask_indices_1 = \\\n",
    "            self.convert_range_image_to_point_cloud(\n",
    "                frame,\n",
    "                range_images,\n",
    "                camera_projections,\n",
    "                range_image_top_pose,\n",
    "                ri_index=1\n",
    "            )\n",
    "        points_1 = np.concatenate(points_1, axis=0)\n",
    "        intensity_1 = np.concatenate(intensity_1, axis=0)\n",
    "        elongation_1 = np.concatenate(elongation_1, axis=0)\n",
    "        mask_indices_1 = np.concatenate(mask_indices_1, axis=0)\n",
    "\n",
    "        points = np.concatenate([points_0, points_1], axis=0)\n",
    "        intensity = np.concatenate([intensity_0, intensity_1], axis=0)\n",
    "        elongation = np.concatenate([elongation_0, elongation_1], axis=0)\n",
    "        mask_indices = np.concatenate([mask_indices_0, mask_indices_1], axis=0)\n",
    "\n",
    "        # timestamp = frame.timestamp_micros * np.ones_like(intensity)\n",
    "\n",
    "        # concatenate x,y,z, intensity, elongation, timestamp (6-dim)\n",
    "        point_cloud = np.column_stack((points, intensity, elongation, mask_indices))\n",
    "\n",
    "        pc_path = f'{self.point_cloud_save_dir}/{self.prefix}' + f{str(file_idx).zfill(3)}{str(frame_idx).zfill(3)}.bin'\n",
    "        point_cloud.astype(np.float32).tofile(pc_path)\n",
    "\n",
    "    def save_label(self, frame, file_idx, frame_idx, cam_sync=False):\n",
    "        pass\n",
    "\n",
    "    def save_pose(self, frame, file_idx, frame_idx):\n",
    "        pass\n",
    "\n",
    "    def save_timestamp(self, frame, file_idx, frame_idx):\n",
    "        pass\n",
    "\n",
    "    def create_folder(self):\n",
    "        \"\"\"Create folder for data preprocessing.\"\"\"\n",
    "        if not self.test_mode:\n",
    "            dir_list1 = [\n",
    "                self.label_all_save_dir,\n",
    "                self.calib_save_dir,\n",
    "                self.pose_save_dir,\n",
    "                self.timestamp_save_dir,\n",
    "            ]\n",
    "            dir_list2 = [self.label_save_dir, self.image_save_dir]\n",
    "            if self.save_cam_sync_labels:\n",
    "                dir_list1.append(self.cam_sync_label_all_save_dir)\n",
    "                dir_list2.append(self.cam_sync_label_save_dir)\n",
    "        else:\n",
    "            dir_list1 = [\n",
    "                self.calib_save_dir, self.pose_save_dir,\n",
    "                self.timestamp_save_dir\n",
    "            ]\n",
    "            dir_list2 = [self.image_save_dir]\n",
    "        if 'testing_3d_camera_only_detection' not in self.load_dir:\n",
    "            dir_list1.append(self.point_cloud_save_dir)\n",
    "        for d in dir_list1:\n",
    "            mmengine.mkdir_or_exist(d)\n",
    "        for d in dir_list2:\n",
    "            for i in range(5):\n",
    "                mmengine.mkdir_or_exist(f'{d}{str(i)}')\n",
    "\n",
    "    def convert_range_image_to_point_cloud(self, frame, range_images, camera_projections, range_image_top_pose, ri_index=0):\n",
    "        \"\"\"Convert range images to point cloud.\n",
    "\n",
    "        Args:\n",
    "            frame (:obj:'Frame'): Open dataset frame.\n",
    "            range_images (dict): Mapping from laser_name to list of two range images corresponding with two returns.\n",
    "            camera_projections (dict): Mapping from laser_name to list of two camera projections corresponding with two returns.\n",
    "            range_image_top_pose (:obj:`Transform`): Range image pixel pose for top lidar.\n",
    "            ri_index (int, optional): 0 for the first return, 1 for the second return. Default: 0.\n",
    "\n",
    "        Returns:\n",
    "            tuple[list[np.ndarray]]: (list of points with shape [N, 3], camera projections of points with shape [N, 6], \n",
    "                                      intensity with shape [N, 1], elongation with shape [N, 1], \n",
    "                                      points' position in the depth map (element offset if points come from the main lidar otherwise -1) with shape[N, 1]).\n",
    "                                      )\n",
    "        \"\"\"\n",
    "        calibrations = sorted(frame.context.laser_calibrations, key=lambda c: c.name)\n",
    "        \n",
    "        points = []\n",
    "        cp_points = []\n",
    "        intensity = []\n",
    "        elongation = []\n",
    "        mask_indices = []\n",
    "\n",
    "        frame_pose = tf.convert_to_tensor(value=np.reshape(np.array(frame.pose.transform), [4, 4]))\n",
    "\n",
    "        #[H, W, 6]\n",
    "        range_image_top_pose_tensor = tf.reshape(\n",
    "            tf.convert_to_tensor(value=range_image_top_pose.data), range_image_top_pose.shape.dims)\n",
    "        \n",
    "        # [H, W, 3, 3]\n",
    "        range_image_top_pose_tensor_rotation = \\\n",
    "            transform_utils.get_rotation_matrix(\n",
    "                range_image_top_pose_tensor[..., 0],\n",
    "                range_image_top_pose_tensor[..., 1],\n",
    "                range_image_top_pose_tensor[..., 2])\n",
    "\n",
    "        range_image_top_pose_tensor_translation = \\\n",
    "            range_image_top_pose_tensor[..., 3:]\n",
    "\n",
    "        range_image_top_pose_tensor = transform_utils.get_transform(\n",
    "            range_image_top_pose_tensor_rotation, range_image_top_pose_tensor_translation\n",
    "        )\n",
    "\n",
    "        for c in calibrations:\n",
    "            range_image = range_images[c.name][ri_index] # <-------\n",
    "\n",
    "            if len(c.beam_inclinations) == 0: # 경사가 0 이라면\n",
    "                beam_inclinations = range_image_utils.compute_inclination(\n",
    "                    tf.constant([c.beam_inclination_min, c.beam_inclination_max]),\n",
    "                                height=range_image.shape.dims[0])\n",
    "            else:\n",
    "                beam_inclinations = tf.constant(c.beam_inclinations)\n",
    "\n",
    "            beam_inclinations = tf.reverse(beam_inclinations, axis=[-1])\n",
    "            extrinsic = np.reshape(np.array(c.extrinsic.transform), [4, 4])\n",
    "\n",
    "            range_image_tensor = tf.reshape(tf.convert_to_tensor(value=range_image.data), # <-------\n",
    "                                            range_image.shape.dims)\n",
    "            pixel_pose_local = None\n",
    "            frame_pose_local = None\n",
    "\n",
    "            if c.name == dataset_pb2.LaserName.TOP:\n",
    "                pixel_pose_local = range_image_top_pose_tensor\n",
    "                pixel_pose_local = tf.expand_dims(pixel_pose_local, axis=0)\n",
    "                frame_pose_local = tf.expand_dims(frame_pose, axis=0)\n",
    "\n",
    "            range_image_mask = range_image_tensor[..., 0] > 0\n",
    "\n",
    "            if self.filter_no_label_zone_points:\n",
    "                nlz_mask = range_image_tensor[..., 3] != 1.0  # 1.0: in NLZ , 1.0이면 노 라벨 존이다??\n",
    "                range_image_mask = range_image_mask & nlz_mask # range_image_mask & 라벨 존이 아닌 곳\n",
    "\n",
    "            range_image_cartesian = \\\n",
    "                range_image_utils.extract_point_cloud_from_range_image( # extract_point_cloud_from_range_image\n",
    "                    tf.expand_dims(range_image_tensor[..., 0], axis=0),\n",
    "                    tf.expand_dims(extrinsic, axis=0),\n",
    "                    tf.expand_dims(tf.convert_to_tensor(value=beam_inclinations), axis=0),\n",
    "                    pixel_pose=pixel_pose_local,\n",
    "                    frame_pose=frame_pose_local)\n",
    "\n",
    "            mask_index = tf.where(range_image_mask)\n",
    "\n",
    "            range_image_cartesian = tf.squeeze(range_image_cartesian, axis=0)\n",
    "            points_tensor = tf.gather_nd(range_image_cartesian, mask_index)\n",
    "\n",
    "            cp = camera_projections[c.name][ri_index] # <-------------------------------------------\n",
    "            cp_tensor = tf.reshape(tf.convert_to_tensor(value=cp.data), cp.shape.dims)\n",
    "\n",
    "            cp_points_tensor = tf.gather_nd(cp_tensor, mask_index)\n",
    "            \n",
    "            points.append(points_tensor.numpy())       # <-------------------------------------------\n",
    "            cp_points.append(cp_points_tensor.numpy())\n",
    "\n",
    "            intensity_tensor = tf.gather_nd(range_image_tensor[..., 1], mask_index)\n",
    "            intensity.append(intensity_tensor.numpy())\n",
    "\n",
    "            elongation_tensor = tf.gather_nd(range_image_tensor[..., 2], mask_index) # elongation_tensor???\n",
    "            elongation.append(elongation_tensor.numpy())\n",
    "\n",
    "            if c.name == 1:\n",
    "                mask_index = (ri_index * range_image_mask.shape[0] + mask_index[:, 0]) *  range_image_mask.shape[1] + mask_index[:, 1]\n",
    "                mask_index = mask_index.numpy().astype(elongation[-1].dtype)\n",
    "            else:\n",
    "                mask_index = np.full_like(elongation[-1], -1)\n",
    "\n",
    "            mask_indices.append(mask_index)\n",
    "\n",
    "        return points, cp_points, intensity, elongation, mask_indices\n",
    "\n",
    "\n",
    "    def cart_to_homo(self, mat):\n",
    "        \"\"\"Convert transformation matrix in Cartesian coordinates to\n",
    "        homogeneous format.\n",
    "\n",
    "        Args:\n",
    "            mat (np.ndarray): Transformation matrix in Cartesian.\n",
    "                The input matrix shape is 3x3 or 3x4.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Transformation matrix in homogeneous format.\n",
    "                The matrix shape is 4x4.\n",
    "        \"\"\"\n",
    "        ret = np.eye(4)\n",
    "        if mat.shape == (3, 3):\n",
    "            ret[:3, :3] = mat\n",
    "        elif mat.shape == (3, 4):\n",
    "            ret[:3, :] = mat\n",
    "        else:\n",
    "            raise ValueError(mat.shape)\n",
    "        return ret\n",
    "        \n",
    "def create_ImageSets_img_ids(root_dir, splits):\n",
    "    save_dir = join(root_dir, 'ImageSets/')\n",
    "    if not exists(save_dir):\n",
    "        os.mkdir(save_dir)\n",
    "\n",
    "    idx_all = [[] for i in splits]\n",
    "    for i, split in enumerate(splits):\n",
    "        path = join(root_dir, splits[i], 'calib')\n",
    "        if not exists(path):\n",
    "            RawNames = []\n",
    "        else:\n",
    "            RawNames = os.listdir(path)\n",
    "\n",
    "        for name in RawNames:\n",
    "            if name.endswith('.txt'):\n",
    "                idx = name.replace('.txt', '\\n')\n",
    "                idx_all[int(idx[0])].append(idx)\n",
    "        idx_all[i].sort()\n",
    "\n",
    "    open(save_dir + 'train.txt', 'w').writelines(idx_all[0])\n",
    "    open(save_dir + 'val.txt', 'w').writelines(idx_all[1])\n",
    "    open(save_dir + 'trainval.txt', 'w').writelines(idx_all[0] + idx_all[1])\n",
    "    open(save_dir + 'test.txt', 'w').writelines(idx_all[2])\n",
    "    # open(save_dir+'test_cam_only.txt','w').writelines(idx_all[3])\n",
    "    print('created txt files indicating what to collect in ', splits)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.17 ('mm3d')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d69b847d396095f32f676329057412de0235e6809d7fbe768ce2e81f9c069d24"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
