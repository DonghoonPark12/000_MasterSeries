{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 포인트 클라우드에서 Frustum에 해당하는 포인트만 남기는 법\n",
    "이하 코드에서는 KITTI PointCloud 처리 방법을 살펴본다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import pdb\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "# CUR = os.path.dirname(os.path.abspath(__file__))\n",
    "# sys.path.append(CUR)\n",
    "CUR = 'C:\\DDrive\\PointPillars'\n",
    "sys.path.append('.')\n",
    "from utils import read_points, write_points, read_calib, read_label, \\\n",
    "    write_pickle, get_points_num_in_bbox, points_in_bboxes_v2\n",
    "    #remove_outside_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copied from https://github.com/open-mmlab/mmdetection3d/blob/f45977008a52baaf97640a0e9b2bbe5ea1c4be34/mmdet3d/core/bbox/box_np_ops.py#L609\n",
    "def projection_matrix_to_CRT_kitti(proj):\n",
    "    \"\"\"\n",
    "        (1) proj 행렬에서 CR, CT를 분리한 다음에\n",
    "        (2) CR에서 C가 상삼각행렬 인 특성을 이용해, QR 분해를 시도한다.\n",
    "            - 여기서 왜 RinvCinv 즉 CR 역행렬을 구하는지??? \n",
    "            - R도 당연 분리가 가능하다.\n",
    "        (3) C가 분리되면 Cinv @ CT로 부터 T도 구한다.\n",
    "        (4) C, R, T 리턴\n",
    "    \"\"\"\n",
    "    \"\"\"Split projection matrix of kitti.\n",
    "    P = C @ [R|T]\n",
    "    C is upper triangular matrix, so we need to inverse CR and use QR\n",
    "    stable for all kitti camera projection matrix.\n",
    "    Args:\n",
    "        proj (p.array, shape=[4, 4]): Intrinsics of camera.\n",
    "    Returns:\n",
    "        tuple[np.ndarray]: Splited matrix of C, R and T.\n",
    "    \"\"\"\n",
    "\n",
    "    CR = proj[0:3, 0:3]\n",
    "    CT = proj[0:3, 3]\n",
    "    RinvCinv = np.linalg.inv(CR)\n",
    "    Rinv, Cinv = np.linalg.qr(RinvCinv)\n",
    "    C = np.linalg.inv(Cinv)\n",
    "    R = np.linalg.inv(Rinv)\n",
    "    T = Cinv @ CT\n",
    "    return C, R, T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copied from https://github.com/open-mmlab/mmdetection3d/blob/f45977008a52baaf97640a0e9b2bbe5ea1c4be34/mmdet3d/core/bbox/box_np_ops.py#L661\n",
    "def get_frustum(bbox_image, C, near_clip=0.001, far_clip=100):\n",
    "    \"\"\"Get frustum corners in camera coordinates.\n",
    "    Args:\n",
    "        bbox_image (list[int]): box in image coordinates.\n",
    "        C (np.ndarray): Intrinsics.\n",
    "        near_clip (float, optional): Nearest distance of frustum.\n",
    "            Defaults to 0.001.\n",
    "        far_clip (float, optional): Farthest distance of frustum.\n",
    "            Defaults to 100.\n",
    "    Returns:\n",
    "        np.ndarray, shape=[8, 3]: coordinates of frustum corners.\n",
    "    \"\"\"\n",
    "    fku = C[0, 0]\n",
    "    fkv = -C[1, 1]\n",
    "    u0v0 = C[0:2, 2]\n",
    "    z_points = np.array(\n",
    "        [near_clip] * 4 + [far_clip] * 4, dtype=C.dtype)[:, np.newaxis]\n",
    "    \"\"\"\n",
    "    >>> z_points\n",
    "    [[1.e-03]\n",
    "    [1.e-03]\n",
    "    [1.e-03]\n",
    "    [1.e-03]\n",
    "    [1.e+02]\n",
    "    [1.e+02]\n",
    "    [1.e+02]\n",
    "    [1.e+02]]\n",
    "    \"\"\"\n",
    "    b = bbox_image\n",
    "    \"\"\"\n",
    "    [0, 0, 1224, 370]\n",
    "    \"\"\"\n",
    "    box_corners = np.array(\n",
    "        [[b[0], b[1]], [b[0], b[3]], [b[2], b[3]], [b[2], b[1]]], dtype=C.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified from https://github.com/open-mmlab/mmdetection3d/blob/f45977008a52baaf97640a0e9b2bbe5ea1c4be34/mmdet3d/core/bbox/box_np_ops.py#L609\n",
    "def remove_outside_points(points, r0_rect, tr_velo_to_cam, P2, image_shape):\n",
    "    \"\"\"Remove points which are outside of image.\n",
    "    Args:\n",
    "        points (np.ndarray, shape=[N, 3+dims]): Total points.\n",
    "        rect (np.ndarray, shape=[4, 4]): Matrix to project points in\n",
    "            specific camera coordinate (e.g. CAM2) to CAM0.\n",
    "        Trv2c (np.ndarray, shape=[4, 4]): Matrix to project points in\n",
    "            camera coordinate to lidar coordinate.\n",
    "        P2 (p.array, shape=[4, 4]): Intrinsics of Camera2.\n",
    "        image_shape (list[int]): Shape of image.\n",
    "    Returns:\n",
    "        np.ndarray, shape=[N, 3+dims]: Filtered points.\n",
    "    \"\"\"\n",
    "    # 5x faster than remove_outside_points_v1(2ms vs 10ms)\n",
    "    C, R, T = projection_matrix_to_CRT_kitti(P2)\n",
    "    image_bbox = [0, 0, image_shape[1], image_shape[0]]\n",
    "    frustum = get_frustum(image_bbox, C)\n",
    "    frustum -= T\n",
    "    frustum = np.linalg.inv(R) @ frustum.T\n",
    "    frustum = points_camera2lidar(frustum.T[None, ...], tr_velo_to_cam, r0_rect) # (1, 8, 3)\n",
    "    group_rectangle_vertexs_v = group_rectangle_vertexs(frustum)\n",
    "    frustum_surfaces = group_plane_equation(group_rectangle_vertexs_v)\n",
    "    indices = points_in_bboxes(points[:, :3], frustum_surfaces) # (N, 1)\n",
    "    points = points[indices.reshape([-1])]\n",
    "    return points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_info_pkl(data_root, data_type, prefix, label=True, db=False):\n",
    "    sep = os.path.sep # OS별로 파일 경로는 구분하는 기준, 윈도우에서는 '/'\n",
    "    print(f\"Processing {data_type} data..\")\n",
    "    ids_file = os.path.join(CUR, 'dataset', 'ImageSets', f'{data_type}.txt')\n",
    "\n",
    "    with open(ids_file, 'r') as f:\n",
    "        ids = [id.strip() for id in f.readlines()]\n",
    "    \n",
    "    split = 'training' if label else 'testing'\n",
    "\n",
    "    kitti_infos_dict = {}\n",
    "\n",
    "    if db: # 학습할 때만 생성한다?\n",
    "        kitti_dbinfos_train = {}\n",
    "        db_points_saved_path = os.path.join(data_root, f'{prefix}_gt_database')\n",
    "        os.makedirs(db_points_saved_path, exist_ok=True)\n",
    "\n",
    "    for id in tqdm(ids): # train 3712, val 3769, test \n",
    "        cur_info_dict={}\n",
    "        img_path = os.path.join(data_root, split, 'image_2', f'{id}.png')\n",
    "        lidar_path = os.path.join(data_root, split, 'velodyne', f'{id}.bin')\n",
    "        calib_path = os.path.join(data_root, split, 'calib', f'{id}.txt') \n",
    "        #print(lidar_path.split(sep)[-3:]) # / 단위로 분리한 다음에 끝에 3개 path를 분할한다.\n",
    "        # e.g. ['training', 'velodyne', '000000.bin']\n",
    "        cur_info_dict['velodyne_path'] = sep.join(lidar_path.split(sep)[-3:])\n",
    "\n",
    "        img = cv2.imread(img_path)    \n",
    "        image_shape = img.shape[:2]\n",
    "\n",
    "        cur_info_dict['image'] = {\n",
    "            'image_shape': image_shape,\n",
    "            'image_path': sep.join(img_path.split(sep)[-3:]), \n",
    "            'image_idx': int(id),\n",
    "        }\n",
    "\n",
    "        calib_dict = read_calib(calib_path)\n",
    "        cur_info_dict['calib'] = calib_dict    \n",
    "\n",
    "        lidar_points = read_points(lidar_path)\n",
    "        \n",
    "        # 본 코드에서 부분. 이미지 밖의 포인트를 제거\n",
    "        reduced_lidar_points = remove_outside_points(\n",
    "            points=lidar_points, \n",
    "            r0_rect=calib_dict['R0_rect'], \n",
    "            tr_velo_to_cam=calib_dict['Tr_velo_to_cam'], \n",
    "            P2=calib_dict['P2'], \n",
    "            image_shape=image_shape)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing train data..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3712 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 1224, 370]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'NoneType' and 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[61], line 30\u001b[0m\n\u001b[0;32m     26\u001b[0m parser\u001b[39m.\u001b[39madd_argument(\u001b[39m'\u001b[39m\u001b[39m--prefix\u001b[39m\u001b[39m'\u001b[39m, default\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mkitti\u001b[39m\u001b[39m'\u001b[39m, \n\u001b[0;32m     27\u001b[0m                     help\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mthe prefix name for the saved .pkl file\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     28\u001b[0m args \u001b[39m=\u001b[39m parser\u001b[39m.\u001b[39mparse_args(args\u001b[39m=\u001b[39m[]) \u001b[39m# 주피터 노트북에서 argparse 사용하려면, () -> args=[]\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m main(args)\n",
      "Cell \u001b[1;32mIn[61], line 8\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(args)\u001b[0m\n\u001b[0;32m      3\u001b[0m prefix \u001b[39m=\u001b[39m args\u001b[39m.\u001b[39mprefix\n\u001b[0;32m      5\u001b[0m \u001b[39m## 1. train: create data infomation pkl file && create reduced point clouds \u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[39m##           && create database(points in gt bbox) for data aumentation\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[39m# pkl 파일 생성, 감소된 포인트 클라우드 생성, 데이터 증강을 위한 db(GT 박스 안의 포인트) 생성\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m kitti_train_infos_dict \u001b[39m=\u001b[39m create_data_info_pkl(data_root, \u001b[39m'\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m'\u001b[39;49m, prefix, db\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m     10\u001b[0m \u001b[39m## 2. val: create data infomation pkl file && create reduced point clouds\u001b[39;00m\n\u001b[0;32m     11\u001b[0m kitti_val_infos_dict \u001b[39m=\u001b[39m create_data_info_pkl(data_root, \u001b[39m'\u001b[39m\u001b[39mval\u001b[39m\u001b[39m'\u001b[39m, prefix)\n",
      "Cell \u001b[1;32mIn[60], line 42\u001b[0m, in \u001b[0;36mcreate_data_info_pkl\u001b[1;34m(data_root, data_type, prefix, label, db)\u001b[0m\n\u001b[0;32m     39\u001b[0m lidar_points \u001b[39m=\u001b[39m read_points(lidar_path)\n\u001b[0;32m     41\u001b[0m \u001b[39m# 본 코드에서 부분. 이미지 밖의 포인트를 제거\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m reduced_lidar_points \u001b[39m=\u001b[39m remove_outside_points(\n\u001b[0;32m     43\u001b[0m     points\u001b[39m=\u001b[39;49mlidar_points, \n\u001b[0;32m     44\u001b[0m     r0_rect\u001b[39m=\u001b[39;49mcalib_dict[\u001b[39m'\u001b[39;49m\u001b[39mR0_rect\u001b[39;49m\u001b[39m'\u001b[39;49m], \n\u001b[0;32m     45\u001b[0m     tr_velo_to_cam\u001b[39m=\u001b[39;49mcalib_dict[\u001b[39m'\u001b[39;49m\u001b[39mTr_velo_to_cam\u001b[39;49m\u001b[39m'\u001b[39;49m], \n\u001b[0;32m     46\u001b[0m     P2\u001b[39m=\u001b[39;49mcalib_dict[\u001b[39m'\u001b[39;49m\u001b[39mP2\u001b[39;49m\u001b[39m'\u001b[39;49m], \n\u001b[0;32m     47\u001b[0m     image_shape\u001b[39m=\u001b[39;49mimage_shape)\n",
      "Cell \u001b[1;32mIn[59], line 19\u001b[0m, in \u001b[0;36mremove_outside_points\u001b[1;34m(points, r0_rect, tr_velo_to_cam, P2, image_shape)\u001b[0m\n\u001b[0;32m     17\u001b[0m image_bbox \u001b[39m=\u001b[39m [\u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, image_shape[\u001b[39m1\u001b[39m], image_shape[\u001b[39m0\u001b[39m]]\n\u001b[0;32m     18\u001b[0m frustum \u001b[39m=\u001b[39m get_frustum(image_bbox, C)\n\u001b[1;32m---> 19\u001b[0m frustum \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m T\n\u001b[0;32m     20\u001b[0m frustum \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mlinalg\u001b[39m.\u001b[39minv(R) \u001b[39m@\u001b[39m frustum\u001b[39m.\u001b[39mT\n\u001b[0;32m     21\u001b[0m frustum \u001b[39m=\u001b[39m points_camera2lidar(frustum\u001b[39m.\u001b[39mT[\u001b[39mNone\u001b[39;00m, \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m], tr_velo_to_cam, r0_rect) \u001b[39m# (1, 8, 3)\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'NoneType' and 'float'"
     ]
    }
   ],
   "source": [
    "def main(args):\n",
    "    data_root = args.data_root\n",
    "    prefix = args.prefix\n",
    "\n",
    "    ## 1. train: create data infomation pkl file && create reduced point clouds \n",
    "    ##           && create database(points in gt bbox) for data aumentation\n",
    "    # pkl 파일 생성, 감소된 포인트 클라우드 생성, 데이터 증강을 위한 db(GT 박스 안의 포인트) 생성\n",
    "    kitti_train_infos_dict = create_data_info_pkl(data_root, 'train', prefix, db=True)\n",
    "\n",
    "    ## 2. val: create data infomation pkl file && create reduced point clouds\n",
    "    kitti_val_infos_dict = create_data_info_pkl(data_root, 'val', prefix)\n",
    "    \n",
    "    ## 3. trainval: create data infomation pkl file\n",
    "    kitti_trainval_infos_dict = {**kitti_train_infos_dict, **kitti_val_infos_dict}\n",
    "    saved_path = os.path.join(data_root, f'{prefix}_infos_trainval.pkl')\n",
    "    write_pickle(kitti_trainval_infos_dict, saved_path)\n",
    "\n",
    "    ## 4. test: create data infomation pkl file && create reduced point clouds\n",
    "    kitti_test_infos_dict = create_data_info_pkl(data_root, 'test', prefix, label=False)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(description='Dataset infomation')\n",
    "    parser.add_argument('--data_root', default='C:/DDrive/000_Dataset/kitti', \n",
    "                        help='your data root for kitti')\n",
    "    parser.add_argument('--prefix', default='kitti', \n",
    "                        help='the prefix name for the saved .pkl file')\n",
    "    args = parser.parse_args(args=[]) # 주피터 노트북에서 argparse 사용하려면, () -> args=[]\n",
    "\n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.17 ('torchWin_rev')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dcfd0bbc3e25ae79d1f134c08b87e3777d49169aec2e55e1b5fb8249822c085d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
