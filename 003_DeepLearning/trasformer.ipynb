{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerLayer(nn.Module):\n",
    "    # Transformer layer https://arxiv.org/abs/2010.11929 (LayerNorm layers removed for better performance)\n",
    "    def __init__(self, c, num_heads):\n",
    "        super().__init__()\n",
    "        self.q = nn.Linear(c, c, bias=False)\n",
    "        self.k = nn.Linear(c, c, bias=False)\n",
    "        self.v = nn.Linear(c, c, bias=False)\n",
    "        self.ma = nn.MultiheadAttention(embed_dim=c, num_heads=num_heads)\n",
    "        self.fc1 = nn.Linear(c, c, bias=False)\n",
    "        self.fc2 = nn.Linear(c, c, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.ma(self.q(x), self.k(x), self.v(x))[0] + x\n",
    "        x = self.fc2(self.fc1(x)) + x\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    # Vision Transformer https://arxiv.org/abs/2010.11929\n",
    "    def __init__(self, c1, c2, num_heads, num_layers):\n",
    "        super().__init__()\n",
    "        self.conv = None\n",
    "        if c1 != c2:\n",
    "            self.conv = Conv(c1, c2)\n",
    "        self.linear = nn.Linear(c2, c2)  # learnable position embedding\n",
    "        self.tr = nn.Sequential(*[TransformerLayer(c2, num_heads) for _ in range(num_layers)])\n",
    "        self.c2 = c2\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.conv is not None:\n",
    "            x = self.conv(x)\n",
    "        b, _, w, h = x.shape\n",
    "        p = x.flatten(2).unsqueeze(0).transpose(0, 3).squeeze(3)\n",
    "        return self.tr(p + self.linear(p)).unsqueeze(3).transpose(0, 3).reshape(b, self.c2, w, h)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.17 ('mm3d')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.17"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d69b847d396095f32f676329057412de0235e6809d7fbe768ce2e81f9c069d24"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
