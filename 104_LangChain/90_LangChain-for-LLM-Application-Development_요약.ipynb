{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain: Models, Prompts and Output Parsers\n",
    "\n",
    "<목차>\n",
    "- 1. Chat API : OpenAI\n",
    "  - Direct API calls to OpenAI\n",
    "- 2. Chat API : LangChain\n",
    "  - chat = ChatOpenAI()\n",
    "  - prompt_template = ChatPromptTemplate.from_template()\n",
    "  - prompt_template.format_messages()\n",
    "- 3. Output parsers\n",
    "  - \n",
    "\n",
    "## Outline\n",
    " * Direct API calls to OpenAI\n",
    " * API calls through LangChain:\n",
    "   * Prompts\n",
    "   * Models\n",
    "   * Output parsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "import sys\n",
    "sys.path.append('./')\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv('.env')) # read local .env file\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-3.5-turbo-0301\n"
     ]
    }
   ],
   "source": [
    "# account for deprecation of LLM model\n",
    "import datetime\n",
    "# Get the current date\n",
    "current_date = datetime.datetime.now().date()\n",
    "\n",
    "# Define the date after which the model should be set to \"gpt-3.5-turbo\"\n",
    "target_date = datetime.date(2024, 6, 12)\n",
    "\n",
    "# Set the model variable based on the current date\n",
    "if current_date > target_date:\n",
    "    llm_model = \"gpt-3.5-turbo\"\n",
    "else:\n",
    "    llm_model = \"gpt-3.5-turbo-0301\"\n",
    "print(llm_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Chat API : OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1+1 equals 2.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0, \n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "get_completion(\"What is 1+1?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translate the text that is delimited by triple backticks \n",
      "into a style that is American English in a calm and respectful tone\n",
      ".\n",
      "text: ```\n",
      "Arrr, I be fuming that me blender lid flew off and splattered me kitchen walls with smoothie! And to make matters worse,the warranty don't cover the cost of cleaning up me kitchen. I need yer help right now, matey!\n",
      "```\n",
      "\n",
      "I'm really frustrated that my blender lid flew off and splattered my kitchen walls with smoothie! And to make matters worse, the warranty doesn't cover the cost of cleaning up my kitchen. I could really use your help right now, friend.\n"
     ]
    }
   ],
   "source": [
    "# 프롬프트 구성\n",
    "customer_email = \"\"\"\n",
    "Arrr, I be fuming that me blender lid \\\n",
    "flew off and splattered me kitchen walls \\\n",
    "with smoothie! And to make matters worse,\\\n",
    "the warranty don't cover the cost of \\\n",
    "cleaning up me kitchen. I need yer help \\\n",
    "right now, matey!\n",
    "\"\"\"\n",
    "style = \"\"\"American English \\\n",
    "in a calm and respectful tone\n",
    "\"\"\"\n",
    "prompt = f\"\"\"Translate the text \\\n",
    "that is delimited by triple backticks \n",
    "into a style that is {style}.\n",
    "text: ```{customer_email}```\n",
    "\"\"\"\n",
    "print(prompt)\n",
    "\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Chat API : LangChain\n",
    "- #!pip install --upgrade langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'llm_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#from langchain_openai import ChatOpenAI\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprompts\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatPromptTemplate\n\u001b[1;32m----> 5\u001b[0m chat \u001b[38;5;241m=\u001b[39m ChatOpenAI(temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m, model \u001b[38;5;241m=\u001b[39m \u001b[43mllm_model\u001b[49m) \u001b[38;5;66;03m# <- client = OpenAI() 에서 다음과 같이 변경\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'llm_model' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "#from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "chat = ChatOpenAI(temperature=0.0, model = llm_model) # <- client = OpenAI() 에서 다음과 같이 변경"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- template를 구성하여 ChatPromptTemplate의 입력으로 넣습니다.\n",
    "\n",
    "==> \n",
    "- ChatPromptTemplate 클래스는 input_variables와 messages를 멤버로 갖습니다.\n",
    "- input_variables의 타입은 list, messages의 타입도 list입니다.\n",
    "- ChatPromptTemplate.input_variables은 ['style', 'text'] list이며\n",
    "- ChatPromptTemplate.messages는 'langchain_core.prompts.chat.HumanMessagePromptTemplate'  클래스의 리스트 입니다.\n",
    "- 'langchain_core.prompts.chat.HumanMessagePromptTemplate' 클래스는 prompt를 멤버로 갖습니다.\n",
    "- `langchain_core.prompts.chat.HumanMessagePromptTemplate.prompt는 class 'langchain_core.prompts.prompt.PromptTemplate' 클래스 객체` 입니다.\n",
    "\n",
    "==> \n",
    "ChatPromptTemplate \n",
    " ㄴinput_variables : ['style', 'text'] 리스트\n",
    " ㄴfrom_template(str)\n",
    " ㄴmessages : [`langchain_core.prompts.chat.HumanMessagePromptTemplate`, ] 리스트\n",
    "    ㄴ prompt: `langchain_core.prompts.prompt.PromptTemplate`\n",
    "        ㄴ input_variables\n",
    "        ㄴ template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt.input_variables:  ['style', 'text']\n",
      "prompt.template:  Translate the text that is delimited by triple backticks into a style that is {style}. text: ```{text}```\n",
      "\n",
      "<class 'pydantic.v1.main.ModelMetaclass'>\n",
      "<class 'list'>\n",
      "<class 'langchain_core.prompts.chat.HumanMessagePromptTemplate'>\n",
      "prompt=PromptTemplate(input_variables=['style', 'text'], template='Translate the text that is delimited by triple backticks into a style that is {style}. text: ```{text}```\\n')\n",
      "<class 'langchain_core.prompts.prompt.PromptTemplate'>\n"
     ]
    }
   ],
   "source": [
    "customer_email = \"\"\"\n",
    "Arrr, I be fuming that me blender lid \\\n",
    "flew off and splattered me kitchen walls \\\n",
    "with smoothie! And to make matters worse, \\\n",
    "the warranty don't cover the cost of \\\n",
    "cleaning up me kitchen. I need yer help \\\n",
    "right now, matey!\n",
    "\"\"\"\n",
    "\n",
    "customer_style = \"\"\"American English \\\n",
    "in a calm and respectful tone\n",
    "\"\"\"\n",
    "\n",
    "template_string = \"\"\"Translate the text \\\n",
    "that is delimited by triple backticks \\\n",
    "into a style that is {style}. \\\n",
    "text: ```{text}```\n",
    "\"\"\"\n",
    "prompt_template = ChatPromptTemplate.from_template(template_string)\n",
    "print('prompt.input_variables: ', prompt_template.messages[0].prompt.input_variables)\n",
    "print('prompt.template: ',prompt_template.messages[0].prompt.template)\n",
    "\n",
    "print(type(ChatPromptTemplate))\n",
    "print(type(prompt_template.input_variables))\n",
    "print(type(prompt_template.messages[0]))\n",
    "print(prompt_template.messages[0])\n",
    "print(type(prompt_template.messages[0].prompt))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- prompt_template의 format_messages 메소드를 사용하여 원하는 `스타일`과 `원본`을 입력으로 넣습니다.\n",
    "- chat_models의 입력은 langchain_core.messages.human.HumanMessage 객체가 들어갑니다.\n",
    "- ChatPromptTemplate 클래스의 messages 멤버는 langchain_core.prompts.chat.HumanMessagePromptTemplate 객체입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessage(content=\"Translate the text that is delimited by triple backticks into a style that is American English in a calm and respectful tone\\n. text: ```\\nArrr, I be fuming that me blender lid flew off and splattered me kitchen walls with smoothie! And to make matters worse, the warranty don't cover the cost of cleaning up me kitchen. I need yer help right now, matey!\\n```\\n\")]\n",
      "<class 'list'>\n",
      "<class 'langchain_core.messages.human.HumanMessage'>\n",
      "Translate the text that is delimited by triple backticks into a style that is American English in a calm and respectful tone\n",
      ". text: ```\n",
      "Arrr, I be fuming that me blender lid flew off and splattered me kitchen walls with smoothie! And to make matters worse, the warranty don't cover the cost of cleaning up me kitchen. I need yer help right now, matey!\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customer_messages = prompt_template.format_messages(\n",
    "    style = customer_style,\n",
    "    text = customer_email\n",
    ")\n",
    "print(customer_messages)\n",
    "print(type(customer_messages))\n",
    "print(type(customer_messages[0]))\n",
    "print(customer_messages[0].content) \n",
    "# customer_messages[0].content에는 앞서 prompt_template 객체 생성시 입력한 prompt_template이 들어가 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm really frustrated that my blender lid flew off and made a mess of my kitchen walls with smoothie. To add to my frustration, the warranty doesn't cover the cost of cleaning up my kitchen. Can you please help me out, friend?\n"
     ]
    }
   ],
   "source": [
    "# chat_models의 입력은 langchain_core.messages.human.HumanMessage 객체가 들어간다.\n",
    "customer_response = chat(customer_messages) \n",
    "print(customer_response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translate the text that is delimited by triple backticks into a style that is a polite tone that speaks in English Pirate. text: ```Hey there customer, the warranty does not cover cleaning expenses for your kitchen because it's your fault that you misused your blender by forgetting to put the lid on before starting the blender. Tough luck! See ya!\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 예시(2)\n",
    "service_reply = \"\"\"Hey there customer, \\\n",
    "the warranty does not cover \\\n",
    "cleaning expenses for your kitchen \\\n",
    "because it's your fault that \\\n",
    "you misused your blender \\\n",
    "by forgetting to put the lid on before \\\n",
    "starting the blender. \\\n",
    "Tough luck! See ya!\n",
    "\"\"\"\n",
    "service_style_pirate = \"\"\"\\\n",
    "a polite tone \\\n",
    "that speaks in English Pirate\\\n",
    "\"\"\"\n",
    "service_messages = prompt_template.format_messages(\n",
    "    style = service_style_pirate,\n",
    "    text = service_reply\n",
    ")\n",
    "print(service_messages[0].content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ahoy there, matey! I must kindly inform ye that the warranty be not coverin' the expenses o' cleaning yer galley, as 'tis yer own fault fer misusin' yer blender by forgettin' to put the lid on afore startin' it. Aye, tough luck! Farewell and may the winds be in yer favor!\n"
     ]
    }
   ],
   "source": [
    "service_response = chat(service_messages) \n",
    "print(service_response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "참조\n",
    "- https://python.langchain.com/docs/modules/model_io/prompts/quick_start#chatprompttemplate\n",
    "- https://python.langchain.com/docs/modules/model_io/output_parsers/quick_start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 추가로 참고할 사이트\n",
    "- https://teddylee777.github.io/langchain/langchain-tutorial-01/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 아래의 모델 출력 기능은 Deprecated 되었다\n",
    "import openai\n",
    "\n",
    "model_list = sorted([m['id'] for m in openai.Model.list()['data']])\n",
    "for m in model_list:\n",
    "    print(m)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
