{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EfficientNetV2 활용하기\n",
    "- https://pytorch.org/vision/main/models/efficientnetv2.html\n",
    "- https://pytorch.org/vision/main/models/generated/torchvision.models.efficientnet_v2_l.html#torchvision.models.efficientnet_v2_l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EfficientNet_V2_L_Weights.IMAGENET1K_V1:\n",
    "\n",
    "- These weights are ported from the original paper. Also available as EfficientNet_V2_L_Weights.DEFAULT.\n",
    "- acc@1 (on ImageNet-1K): 85.808\n",
    "- acc@5 (on ImageNet-1K): 97.788\n",
    "- categories: tench, goldfish, great white shark, … (997 omitted)\n",
    "- min_size: height=33, width=33\n",
    "- recipe : https://github.com/pytorch/vision/tree/main/references/classification#efficientnet-v2\n",
    "- num_params: 118,515,272\n",
    "- GFLOPS: 56.08\n",
    "- File size: 454.6 MB\n",
    "\n",
    "The inference transforms are available at EfficientNet_V2_L_Weights.IMAGENET1K_V1.transforms and perform the following preprocessing operations: Accepts PIL.Image, batched (B, C, H, W) and single (C, H, W) image torch.Tensor objects. The images are resized to resize_size=[480] using interpolation=InterpolationMode.BICUBIC, followed by a central crop of crop_size=[480]. Finally the values are first rescaled to [0.0, 1.0] and then normalized using mean=[0.5, 0.5, 0.5] and std=[0.5, 0.5, 0.5]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ViT 활용하기\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q. `ViT_H_14_Weights.IMAGENET1K_SWAG_E2E_V1` 와 `ViT_H_14_Weights.IMAGENET1K_SWAG_LINEAR_V1` 차이는??\n",
    "- `ViT_H_14_Weights.IMAGENET1K_SWAG_E2E_V1` 는 E2E finetuning 오리지널 SWAG 가중치 이다. ViT_H_14_Weights.DEFAULT와 동일\n",
    "- `ViT_H_14_Weights.IMAGENET1K_SWAG_LINEAR_V1` 는 고정된 SWAG trunk 가중치와 ImageNet-1K 데이터에서 학습된 선형 분류기로 구성\n",
    "\n",
    "\n",
    "이러한 가중치는 원래 고정된 SWAG 트렁크 가중치와 ImageNet-1K 데이터에서 학습된 선형 분류기로 구성됩니다. 이러한 가중치는 원래 고정된 SWAG 트렁크 가중치와 ImageNet에서 학습된 선형 분류기로 구성됩니다. -1K 데이터."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ViT_H_14_Weights.IMAGENET1K_SWAG_E2E_V1:\n",
    "- acc@1 (on ImageNet-1K) 88.552\n",
    "- acc@5 (on ImageNet-1K) 98.694\n",
    "- num_params 633470440\n",
    "- (518, 518) 입력으로 넣지 않으면 아래 에러를 맞이한다.\n",
    "AssertionError: Wrong image height! Expected 518 but got 224!\n",
    "\n",
    "ViT_H_14_Weights.IMAGENET1K_SWAG_LINEAR_V1:\n",
    "- acc@1 (on ImageNet-1K) 85.708\n",
    "- acc@5 (on ImageNet-1K) 97.73 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ViT_L_32_Weights.IMAGENET1K_V1:\n",
    "- acc@1 (on ImageNet-1K) 76.972\n",
    "- acc@5 (on ImageNet-1K) 93.07\n",
    "\n",
    "ViT_L_16_Weights.IMAGENET1K_V1:\n",
    "- acc@1 (on ImageNet-1K) 79.662\n",
    "- acc@5 (on ImageNet-1K) 94.638\n",
    "- num_params 304326632\n",
    "\n",
    "- 추론 변환은 ViT_L_16_Weights.IMAGENET1K_V1.transforms 을 통해 가능하다(?)\n",
    "  - 배치 경우 BCHW, 단일 이미지 CHW를 읽어 들이고\n",
    "  - 242로 리사이즈 및 224로 센터 크롭\n",
    "  - [0.0 ~ 1.0] 이며 mean=[0.485, 0.456, 0.406] and std=[0.229, 0.224, 0.225]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q. 이게 변환 함수 까지 제공해준다는 의미인가??\n",
    "A. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
