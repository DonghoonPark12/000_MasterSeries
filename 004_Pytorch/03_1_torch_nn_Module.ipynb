{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<요약>\n",
    "- CLASS torch.nn.Module(*args, **kwargs)\n",
    "- 변수\n",
    "  - training (bool): 해당 모듈 학습 여부를 Boolean으로 나타낸다. \n",
    "  - add_module(name, module): 현재 모듈에 자식 모듈을 삽입한다. 모듈은 name을 통해 속성처럼 접근할 수 있다.\n",
    "  - apply(fn): fn를 자신(self)를 포함한 모든 서브 모델에 재귀적으로 적용한다. [Code]\n",
    "  - *bfloat16()*: 파라미터와 버퍼를 bfloat16 타입으로 캐스팅 한다\n",
    "  - `buffers(recurse=True): 모듈 버퍼에 대한 이터레이터 리턴` [Code]\n",
    "  - `children(): 자식 모듈의 이터레이터를 반환`\n",
    "  - compile(*args, **kwargs): 모듈의 forward를 torch.compile()를 이용해서 컴파일 한다. 이 모듈의 __call__ 메소드가 컴파일 되고, 모든 인수가 torch.compile()에 전달된다.\n",
    "  - cpu(): 파라미터와 버퍼를 cpu로 이동\n",
    "  - gpu(): 파라미터와 버퍼를 gpu로 이동. 또한, 파라미터와 버퍼를 다른 객체로 만든다. `따라서, 모듈이 최적화 되는 동안 GPU에 있는 경우 Optimzer를 구성하기전에 호출되어야 한다(?)`\n",
    "  - *double()*: 파라미터와 버퍼를 double() 타입으로 캐스팅 한다.\n",
    "  - `eval(): 모듈을 평가 모드로 설정, 특정 모듈에만 영향을 미친다.` [Code]\n",
    "  - extra_repr(): 모듈의 추가 표현 설정, 사용자 정의된 추가 정보를 인쇄하려면 자체 모듈에서 이 메서드를 다시 구현해야 합니다. 한 줄 문자열과 여러 줄 문자열 모두 허용.\n",
    "  - *float()*: 파라미터와 버퍼를 float() 타입으로 캐스팅 한다.\n",
    "  - forward(*input): 모든 호출에서 수행되는 계산 정의. 모든 하위 클래스에서 Overriden 되어야 한다.\n",
    "    - 전방향에 대한 레시피는 이 함수 내에서 정의되어야 하지만, Former는 등록된 Hooks 실행을 처리하고 Latter는 자동으로 무시되므로 나중에 Module 객체를 호출해야 한다(?) \n",
    "  - get_buffer(target): target이 제공한 버퍼가 있으면 반환하고, 그렇지 않으면 오류 발생\n",
    "  - get_extra_state()\n",
    "  - get_parameter(target): target이 제공한 파라미터가 있으면 반환하고, 그렇지 않으면 오류 발생\n",
    "  - get_submodule(target): target이 제공한 서브모듈이 있으면 반환하고, 그렇지 않으면 오류 발생\n",
    "  - register_buffer(name, tensor, persistent=True): 모듈에 버퍼를 추가 [Code]\n",
    "  - ..\n",
    "  - modules(): 네트워크의 모든 모듈의 이터레이터 반환 [Code]\n",
    "  - ..\n",
    "  - zero_grad(set_to_none=True): 모든 모델 파라미터의 기울기를 재 성정. torch.optim.Optimizer와 유사 기능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<알아야 하는 것>\n",
    "- modules(), parameters(), buffers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CLASS torch.nn.Module(*args, **kwargs)\n",
    "- https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.register_buffer\n",
    "- 모든 뉴럴 네트워크 모듈의 베이스 클래스이다.\n",
    "- 구현자의 모델은 모두 본 클래스의 자식 클래스이다.\n",
    "- 모듈은 다른 모듈을 포함(contain)할 수 있고, 이 들을 트리 구조로 중첩 시킬수 있다. 서브 모듈을 attributes 처럼 사용할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5)\n",
    "        self.conv2 = nn.Conv2d(20, 20, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        return F.relu(self.conv2(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 서브 모듈(위의 예제에서는 nn.Conv2d())도 등록이 되며 to() 를 이용하여 호출할 때 해당 매개변수도 변환된다.\n",
    "- 상위 클래스(nn.Module)에 대한 호출 (super().__init__())은 하위 클래스 생성 전에 이뤄저야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 변수  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`apply(fn)`\n",
    "- fn를 자신(self)를 포함한 모든 서브 모델에 재귀적으로 적용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "  (1): Linear(in_features=2, out_features=2, bias=True)\n",
      ")\n",
      "\n",
      "\n",
      "Linear(in_features=2, out_features=2, bias=True)\n",
      "Parameter containing:\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True) <-----\n",
      "Linear(in_features=2, out_features=2, bias=True)\n",
      "Parameter containing:\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True) <-----\n",
      "Sequential(\n",
      "  (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "  (1): Linear(in_features=2, out_features=2, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=2, out_features=2, bias=True)\n",
       "  (1): Linear(in_features=2, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def init_weights(m):\n",
    "    print(m)\n",
    "    if type(m) == nn.Linear:\n",
    "        m.weight.fill_(1.0)\n",
    "        print(m.weight, '<-----')\n",
    "net = nn.Sequential(nn.Linear(2,2), nn.Linear(2,2))\n",
    "print(net)\n",
    "print('\\n')\n",
    "net.apply(init_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `buffers(recurse=True)`\n",
    "- 모듈 버퍼에 대한 이터레이터 리턴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()\n",
    "for buf in model.buffers():\n",
    "    print(type(buf, buf.size()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `eval()`\n",
    "- 모듈을 평가 모드로 설정\n",
    "- Dropout, BatchNorm, etc 과 같은 특정 모듈에만 영향을 미칩니다. 동작에 대한 자세한 내용은 특정 모듈의 문서를 참조.\n",
    "- train(False)와 등가이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `get_submodule(target)`\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `named_parameters(prefix='', recurse=True, remove_duplicate=True)`\n",
    "- 모듈 파라미터의 이터레이터를 리턴한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `modules()`\n",
    "- 네트워크 상의 모든 모듈의 이터레이터를 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "  (1): Linear(in_features=2, out_features=2, bias=True)\n",
      ")\n",
      "--------------------------------------------------\n",
      "0 -> Sequential(\n",
      "  (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "  (1): Linear(in_features=2, out_features=2, bias=True)\n",
      ")\n",
      "1 -> Linear(in_features=2, out_features=2, bias=True)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "l = nn.Linear(2, 2)\n",
    "net = nn.Sequential(l, l)\n",
    "print(net)\n",
    "\n",
    "print('-' * 50)\n",
    "\n",
    "for idx, m in enumerate(net.modules()):\n",
    "    print(idx, '->', m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `register_buffer(name, tensor, persistent=True)`\n",
    "- 모듈에 버퍼를 추가\n",
    "- 파라미터로 간주되어서는 안되는 버퍼를 등록\n",
    "  - 예를 들어 BatchNorm의 `running_mean`은 매개 변수가 아니지만, 모듈 상태의 일부.\n",
    "  - 버퍼는 디폴트로 영구적이며, 파라미터와 함께 저장된다.\n",
    "- 영구 버퍼와 비영구 버퍼(persistent=False)의 유일한 차이점은 (persistent=False) 하는 경우 버퍼는 state_dict에 포함되지 않게 된다.\n",
    "- 버퍼는 파라미터와 마찬가지로 이름을 사용하여 속성 처럼 접근할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.register_buffer('running_mean', torch.zeros(num_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `modules()`\n",
    "- 네트워크의 모든 모듈의 이터레이터 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -> Sequential(\n",
      "  (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "  (1): Linear(in_features=2, out_features=2, bias=True)\n",
      ")\n",
      "1 -> Linear(in_features=2, out_features=2, bias=True)\n"
     ]
    }
   ],
   "source": [
    "l = nn.Linear(2, 2)\n",
    "net = nn.Sequential(l, l)\n",
    "for idx, m in enumerate(net.modules()):\n",
    "    print(idx, '->', m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `zero_grad(set_to_none=True)`\n",
    "- 모든 모델 파라미터의 기울기를 재 성정. torch.optim.Optimizer와 유사 기능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q. 모듈과 파라미터 차이??\n",
    "- 모듈은 모델을 구성하는 클래스 집합 인 것으로 보이고,\n",
    "- 파라미터는 학습 가능한 파라미터를 의미한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "model = torchvision.models.resnet18(pretrained=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable ResNet object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\00_PILSA\\000_MasterSeries\\004_Pytorch\\torch_nn.ipynb 셀 13\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/00_PILSA/000_MasterSeries/004_Pytorch/torch_nn.ipynb#X20sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m idx, m \u001b[39min\u001b[39;00m model\u001b[39m.\u001b[39mmodules():\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/00_PILSA/000_MasterSeries/004_Pytorch/torch_nn.ipynb#X20sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39mprint\u001b[39m(m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/00_PILSA/000_MasterSeries/004_Pytorch/torch_nn.ipynb#X20sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m \u001b[39m*\u001b[39m \u001b[39m50\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot unpack non-iterable ResNet object"
     ]
    }
   ],
   "source": [
    "for idx, m in model.modules(): # named_modules()로 호출하지 않으니 불러 오지 못한다??\n",
    "    print(m)\n",
    "    print('=' * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for idx, m in model.named_modules():\n",
    "    print(m)\n",
    "    print('=' * 50)\n",
    "    for param in m.parameters():\n",
    "        print(type(param), param.size())\n",
    "    print('=' * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Register_Buffer 상세\n",
    "- nn.Module.register_buffer('attribute_name', t)\n",
    "  - 모듈 내에서 tensor t는 self.attribute_name 으로 접근 가능하다.\n",
    "  - Tensor t는 학습되지 않는다. (중요)\n",
    "  - model.cuda() 시에 t도 함께 GPU로 간다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "cpu\n",
      "cpu\n",
      "\n",
      "OrderedDict([('param', tensor([[ 0.3470, -1.1370],\n",
      "        [-1.2748,  1.4021]])), ('buff', tensor([[-0.1974, -0.8364],\n",
      "        [ 0.8150,  1.5108]]))])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.param = nn.Parameter(torch.randn([2, 2]))\n",
    "        \n",
    "        self.register_buffer('buff', torch.randn([2, 2]))\n",
    "\n",
    "        self.non_buff = torch.rand([2, 2])\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "model = Model()\n",
    "print(model.param.device)\n",
    "print(model.buff.device)\n",
    "print(model.non_buff.device)\n",
    "\n",
    "print()\n",
    "\n",
    "print(model.state_dict())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "cuda:0\n",
      "cpu\n",
      "\n",
      "OrderedDict([('param', tensor([[ 0.3470, -1.1370],\n",
      "        [-1.2748,  1.4021]], device='cuda:0')), ('buff', tensor([[-0.1974, -0.8364],\n",
      "        [ 0.8150,  1.5108]], device='cuda:0'))])\n"
     ]
    }
   ],
   "source": [
    "model.cuda()\n",
    "print(model.param.device)\n",
    "print(model.buff.device)     # buffer는 GPU로 넘어가지만, 학습에 참여하진 않는다!!\n",
    "print(model.non_buff.device) # 파라미터만 GPU에 넘어간다!!\n",
    "\n",
    "print()\n",
    "\n",
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param\n",
      "Parameter containing:\n",
      "tensor([[ 0.3470, -1.1370],\n",
      "        [-1.2748,  1.4021]], device='cuda:0', requires_grad=True)\n",
      "tensor([[ 0.3470, -1.1370],\n",
      "        [-1.2748,  1.4021]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name) # 'param'만 parameter 이다. \n",
    "    print(param)\n",
    "    print(param.data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "buff\n",
      "tensor([[-0.1974, -0.8364],\n",
      "        [ 0.8150,  1.5108]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for name, buff in model.named_buffers():\n",
    "    print(name) # 'buff'만 buffer 이다.\n",
    "    print(buff) # gpu에 적재 되었지만, 학습 대상이 아니다. optimizer로 업데이트 되지 않는다. requires_grad=True가 없다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.17 ('mm3d')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d69b847d396095f32f676329057412de0235e6809d7fbe768ce2e81f9c069d24"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
