{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AUTOMATIC MIXED PRECISION PACKAGE - TORCH.AMP\n",
    "- torch.amp는 혼합 정밀도를 위한 편리한 방법 제공\n",
    "- [배경] \n",
    "  - 일부 연산은 torch.float32 (float) 데이터 유형을 사용하고, 다른 연산은 더 낮은 정밀도의 부동 소수점 데이터 유형(lower_precision_fp)을 사용하는 경우가 있다.\n",
    "  - torch.float16(half) 또는 torch.bfloat16. `선형 레이어` 및 `컨볼루션`과 같은 일부 작업은 lower_precision_fp에서 훨씬 빠릅니다. \n",
    "  - `Reduction` 같은 다른 작업에는 float32의 동적 범위가 필요한 경우가 많다.\n",
    "  - `혼합 정밀도는 각 작업을 적절한 데이터 유형과 일치시키려고` 한다.\n",
    "- https://pytorch.org/docs/stable/amp.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 일반적으로 데이터 유형이 torch.float16 인 “automatic mixed precision training” 은 `torch.autocast` 와 `torch.cuda.amp.GradScaler` 를 함께 사용한다.\n",
    "  - https://pytorch.org/docs/stable/notes/amp_examples.html#amp-examples\n",
    "  - https://pytorch.org/tutorials/recipes/recipes/amp_recipe.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 그러나 torch.autocast 및 torch.cuda.amp.GradScaler는 모듈식(modular)이므로 원하는 경우 별도로 사용할 수 있다. \n",
    "- `torch.autocast의 CPU 예제 섹션에서 볼 수 있듯이, 데이터 유형이 torch.bfloat16인 CPU의 \"automatic mixed precision training/inference\"은 torch.autocast만 사용`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- CUDA 및 CPU의 경우 API도 별도로 제공한다.\n",
    "  - `torch.autocast(\"cuda\", args...)는 torch.cuda.amp.autocast(args...)와 동일`.\n",
    "  - `torch.autocast(\"cpu\", args...)는 torch.cpu.amp.autocast(args...)와 동일`.\n",
    "  - (CPU의 경우 현재는 torch.bfloat16의 낮은 정밀도 부동 소수점 데이터 유형만 지원)\n",
    "- torch.autocast 및 torch.cpu.amp.autocast는 버전 1.10의 새로운 기능이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Autocasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### CLASS torch.autocast(device_type, dtype=None, enabled=True, cache_enabled=None)\n",
    "- autocast 인스턴스는 스크립트 영역이 mixed precision 으로 실행될 수 있도록 하는 Context Manager 혹은 데코레이터 역할을 한다.\n",
    "- (정확성을 유지하면서, 성능을 향상 시키기 휘해) autocast에서 선택한 작업별 dtype으로 작업이 실행된다.\n",
    "  - https://pytorch.org/docs/stable/amp.html#autocast-op-reference\n",
    "- Autocast 영역에서는 텐서는 어떠한 유형이 될 수 있다. 해당 영역에서는 모델 입력에 대해 half() 혹은 bfloat16()를 호출하면 안된다.\n",
    "- `Autocast는 순전파만 래핑해야 한다. 역전파는 순전파에서 사용되는 자동 캐스팅과 동일한 유형으로 실행된다`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Scaling\n",
    "- 특정 연산에 대해 순전파가 float16 입력인 경우, 역전파 연산은 float16 기울기를 계산한다.\n",
    "- 기울기 값이 매우 작은 경우, float16으로 표현되지 않을 수 있다(?!)\n",
    "- 이 값은 0으로 flust('underflow') 되므로 해당 매개변수에 대한 업데이트가 손실된다.\n",
    "  \n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "`CLASS torch.cuda.amp.GradScaler(init_scale=65536.0, growth_factor=2.0, backoff_factor=0.5, growth_interval=2000, enabled=True)`\n",
    "- "
   ]
=======
    "#### `CLASS torch.cuda.amp.GradScaler(init_scale=65536.0, growth_factor=2.0, backoff_factor=0.5, growth_interval=2000, enabled=True)`\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
>>>>>>> c8466e689ed75b6312593f5de4dce27dd5743d46
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
