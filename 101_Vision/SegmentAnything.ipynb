{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<목차>\n",
    "- Segmetation을 위한 Foundation Model 모델 설계를 위한 Task 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 이미지 Segmentation 모델(SAM)을 만들기 위한 Foundation Model을 설계하기 위해서 한 3가지 질문\n",
    "1. What task will enable zero-shot generalization?  \n",
    "2. What is the corresponding model architecture?\n",
    "3. What data can power this task and model? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (처음 본 사물도 인식할 수 있는) zero-shot 일반화를 위한 Task는 어떤 Task인가?  \n",
    "  --> `어떻하면 애매모호한 상황도 모두 대처할 수 있는 모델을 만들 수 있을까? Promptable Segmentation Task를 정의`.  \n",
    "      점(point), 박스(box), 마스크(mask), 텍스트(text)를 입력으로 받을 수 있게 설계.  \n",
    "  --> `모호한 Prompt가 주어졌을 때도 합리적인 mask를 출력해야 한다. 모호하다면 연관된 것을 다 확률로 나타낸다`.\n",
    "  \n",
    "- 이에 상응하는 모델 구조는 어떻게 되야 하는가?  \n",
    "  --> Promt를 당연히 지원해야 한다.\n",
    "  \n",
    "- 어떤 데이터가 필요한가?  \n",
    "  --> Promptable Segmentation Task를 위한 Segmentation Mask는 구하기 어렵다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 수집 과정\n",
    "1. `Assisted-manual`\n",
    "   - 공개된 Segmentation Dataset을 이용해 SAM 초기 학습\n",
    "   - 웹 기반 인터페이스에서 초기 학습된 SAM을 이용해 데이터 생성\n",
    "   - 새로 취득한 Data로 점진적 모델 학습(6회)\n",
    "   - 120k 이미지로 부터 4.3M Mask 취득  \n",
    "2. `Semi-automatic`\n",
    "   - Mask의 종류를 다양화 하는 것을 목표로 함\n",
    "   - 1단계에서 학습된 신뢰도 높은 Mask를 작업 화면에 표시\n",
    "   - Annotator들은 그 외 Object를 작업\n",
    "   - 새로 취득한 Data로 점진적 모델 학습(5회)\n",
    "   - 180k 이미지로 부터 5.9M Mask 취득  \n",
    "3. `Fully automatic`\n",
    "   - 완전 자동화된 Annotation 단계\n",
    "   - 이미지에 32 x 32 Regular Grid Point를 각 포인트 마다 대응되는 Mask가 할당된다.\n",
    "   - 그 중에서 IoU가 높은 Mask만 남긴다.\n",
    "   - 중복된 Mask 제거 등 후처리 작업 진행\n",
    "   - 1.1M 이미지로 부터 1.1B Mask 취득"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 구조\n",
    "\n",
    "![image.png](https://github.com/facebookresearch/segment-anything/raw/main/assets/model_diagram.png?raw=true)\n",
    "- Image encoder output인 image embedding, Prompt encoder output인 prompt embedding을 `두개의 embedding을 Lightweight Mask decoder에서 결합하여 Mask 예측`\n",
    "- Prompt encoder + Mask decoder는 50ms 이내로 수행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Powerful Image Encoder\n",
    "- Prompt Encoder: Sparse/Dense Prompt로 나뉜다.\n",
    "  - Sparse Prompt : Points, Boxes, Text를 임베딩 한다.\n",
    "  - Dense Prompt : Mask을 임베딩 한다.\n",
    "- Mask Decoder\n",
    "\n",
    "※ 참고로 오픈소스에서는 Text를 임베딩하는 기능은 없다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (267457045.py, line 11)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 11\u001b[1;36m\u001b[0m\n\u001b[1;33m    super().__init__()\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "# D:\\00_PILSA\\segment-anything\\segment_anything\\modeling\\prompt_encoder.py\n",
    "class PromptEncoder(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim: int,\n",
    "        image_embedding_size: Tuple[int, int],\n",
    "        input_image_size: Tuple[int, int],\n",
    "        mask_in_chans: int,\n",
    "        activation: Type[nn.Module] = nn.GELU,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "    \n",
    "    def _get_batch_size(\n",
    "        self,\n",
    "        points: Optional[Tuple[torch.Tensor, torch.Tensor]],\n",
    "        boxes: Optional[torch.Tensor],\n",
    "        masks: Optional[torch.Tensor],\n",
    "    ) -> int:\n",
    "        \"\"\"Gets the batch size of the output given the batch size of the input\n",
    "        prompts.\"\"\"\n",
    "        if points is not None:\n",
    "            return points[0].shape[0]\n",
    "        elif boxes is not None:\n",
    "            return boxes.shape[0]\n",
    "        elif masks is not None:\n",
    "            return masks.shape[0]\n",
    "        else:\n",
    "            return 1\n",
    "\n",
    "\n",
    "\n",
    "    def _embed_masks(self, masks: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Embeds mask inputs.\"\"\"\n",
    "        mask_embedding = self.mask_downscaling(masks)\n",
    "        return mask_embedding\n",
    "\n",
    "    def _get_dive(self) -> torch.device:\n",
    "        return self.point_embeddings[0].weight.device\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        points: Optional[Tuple[torch.Tensor, torch.Tensor]],\n",
    "        boxes: Optional[torch.Tensor],\n",
    "        masks: Optional[torch.Tensor],\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Embeds different types of prompts, returning both sparse and dense\n",
    "        embeddings.\n",
    "\n",
    "        Arguments:\n",
    "          points (tuple(torch.Tensor, torch.Tensor) or none): point coordinates\n",
    "            and labels to embed.\n",
    "          boxes (torch.Tensor or none): boxes to embed\n",
    "          masks (torch.Tensor or none): masks to embed\n",
    "\n",
    "        Returns:\n",
    "          torch.Tensor: sparse embeddings for the points and boxes, with shape\n",
    "            BxNx(embed_dim), where N is determined by the number of input points\n",
    "            and boxes.\n",
    "            : Points, Boxes의 sparse 임베딩\n",
    "          torch.Tensor: dense embeddings for the masks, in the shape\n",
    "            Bx(embed_dim)x(embed_H)x(embed_W)\n",
    "            : Mask 임베딩\n",
    "    \"\"\"\n",
    "    bs = self._get_batch_size(points, boxes, masks)\n",
    "    sparse_embeddings = torch.empty((bs, 0, self.embed_dim), device=self._get_device())\n",
    "\n",
    "    if points is not None:\n",
    "        coords, labels = points\n",
    "        point_embeddings = self._embed_points(coords, labels, pad=(boxes is None))\n",
    "        sparse_embeddings = torch.cat([sparse_embeddings, point_embeddings], dim=1)\n",
    "    if boxes is not None:\n",
    "        box_embeddings = self._embed_boxes(boxes)\n",
    "        sparse_embeddings = torch.cat([sparse_embeddings, box_embeddings], dim=1)\n",
    "    \n",
    "    if masks is not None:\n",
    "        dense_embeddings = self._embed_masks(masks)\n",
    "    else:\n",
    "        dense_embeddings = self.no_mask_embed.weight.reshape(1, -1, 1, 1)\n",
    "            .expand(bs, -1, self.image_embedding_size[0], self.image_embedding_size[1])\n",
    "    return sparse_embeddings, dense_embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<예시>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.17 ('mm3d')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d69b847d396095f32f676329057412de0235e6809d7fbe768ce2e81f9c069d24"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
