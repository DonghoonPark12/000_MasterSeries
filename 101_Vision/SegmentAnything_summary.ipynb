{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<목차>\n",
    "- Image Segmetation을 위한 Foundation Model 모델 설계를 위한 Task 정의\n",
    "- 데이터 수집 과정\n",
    "  - Assisted-manual\n",
    "  - Semi-automatic\n",
    "  - Fully-automatic\n",
    "- 모델 구조\n",
    "  - Image Encoder\n",
    "  - Prompt Encoder\n",
    "  - Mask Decoder\n",
    "  - 성능\n",
    "- Everything 기능이란?\n",
    "- 인상적인 후속 주제들\n",
    "- Predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Segmetation 기능을 하는 Foundation Model 모델 설계를 위한 Task 정의\n",
    "이미지 Segmentation 모델(SAM)을 만들기 위한 Foundation Model을 설계하기 위해서 한 3가지 질문\n",
    "1. What task will enable zero-shot generalization?  \n",
    "2. What is the corresponding model architecture?\n",
    "3. What data can power this task and model? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (처음 본 사물도 인식할 수 있는) zero-shot 일반화를 위한 Task는 어떤 Task인가?  \n",
    "  --> `어떻하면 애매모호한 상황도 모두 대처할 수 있는 모델을 만들 수 있을까? Promptable Segmentation Task를 정의`.  \n",
    "      점(point), 박스(box), 마스크(mask), 텍스트(text)를 입력으로 받을 수 있게 설계.  \n",
    "  --> `모호한 Prompt가 주어졌을 때도 합리적인 mask를 출력해야 한다. 모호하다면 연관된 것을 다 확률로 나타낸다`.\n",
    "  \n",
    "- 이에 상응하는 모델 구조는 어떻게 되야 하는가?  \n",
    "  --> Promt를 당연히 지원해야 한다.\n",
    "  \n",
    "- 어떤 데이터가 필요한가?  \n",
    "  --> Promptable Segmentation Task를 위한 Segmentation Mask는 구하기 어렵다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 수집 과정\n",
    "1. `Assisted-manual`\n",
    "   - 공개된 Segmentation Dataset을 이용해 SAM 초기 학습\n",
    "   - 웹 기반 인터페이스에서 초기 학습된 SAM을 이용해 데이터 생성\n",
    "   - 새로 취득한 Data로 점진적 모델 학습(6회)\n",
    "   - 120k 이미지로 부터 4.3M Mask 취득  \n",
    "2. `Semi-automatic`\n",
    "   - Mask의 종류를 다양화 하는 것을 목표로 함\n",
    "   - 1단계에서 학습된 신뢰도 높은 Mask를 작업 화면에 표시\n",
    "   - Annotator들은 그 외 Object를 작업\n",
    "   - 새로 취득한 Data로 점진적 모델 학습(5회)\n",
    "   - 180k 이미지로 부터 5.9M Mask 취득  \n",
    "3. `Fully automatic`\n",
    "   - 완전 자동화된 Annotation 단계\n",
    "   - 이미지에 32 x 32 Regular Grid Point를 각 포인트 마다 대응되는 Mask가 할당된다.\n",
    "   - 그 중에서 IoU가 높은 Mask만 남긴다.\n",
    "   - 중복된 Mask 제거 등 후처리 작업 진행\n",
    "   - 1.1M 이미지로 부터 1.1B Mask 취득"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 구조\n",
    "\n",
    "![image.png](https://github.com/facebookresearch/segment-anything/raw/main/assets/model_diagram.png?raw=true)\n",
    "- Image encoder output인 image embedding, Prompt encoder output인 prompt embedding을 `두개의 embedding을 Lightweight Mask decoder에서 결합하여 Mask 예측`\n",
    "- Prompt encoder + Mask decoder는 50ms 이내로 수행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Powerful Image Encoder\n",
    "- Prompt Encoder: Sparse/Dense Prompt로 나뉜다.\n",
    "  - Sparse Prompt : Points, Boxes, Text를 임베딩 한다.\n",
    "  - Dense Prompt : Mask을 임베딩 한다.\n",
    "- Mask Decoder\n",
    "\n",
    "※ 참고로 오픈소스에서는 Text를 임베딩하는 기능은 없다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prompt Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (267457045.py, line 11)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 11\u001b[1;36m\u001b[0m\n\u001b[1;33m    super().__init__()\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "# segment-anything\\segment_anything\\modeling\\prompt_encoder.py\n",
    "class PromptEncoder(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim: int,\n",
    "        image_embedding_size: Tuple[int, int],\n",
    "        input_image_size: Tuple[int, int],\n",
    "        mask_in_chans: int,\n",
    "        activation: Type[nn.Module] = nn.GELU,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "    \n",
    "        self.mask_downscaling = nn.Sequential(\n",
    "            nn.Conv2d(1, mask_in_chans // 4, kernel_size=2, stride=2),\n",
    "            LayerNorm2d(mask_in_chans//4),\n",
    "            activation(),\n",
    "            nn.Conv2d(mask_in_chans // 4, mask_in_chans, kernel_size=2, stride=2),\n",
    "            LayerNorm2d(mask_in_chans),\n",
    "            activation()\n",
    "            nn.Conv2d(mask_in_chans, embed_dim, kernel_size=1),\n",
    "        )\n",
    "\n",
    "    def _get_batch_size(\n",
    "        self,\n",
    "        points: Optional[Tuple[torch.Tensor, torch.Tensor]],\n",
    "        boxes: Optional[torch.Tensor],\n",
    "        masks: Optional[torch.Tensor],\n",
    "    ) -> int:\n",
    "        \"\"\"Gets the batch size of the output given the batch size of the input\n",
    "        prompts.\"\"\"\n",
    "        if points is not None:\n",
    "            return points[0].shape[0]\n",
    "        elif boxes is not None:\n",
    "            return boxes.shape[0]\n",
    "        elif masks is not None:\n",
    "            return masks.shape[0]\n",
    "        else:\n",
    "            return 1\n",
    "\n",
    "\n",
    "\n",
    "    def _embed_masks(self, masks: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Embeds mask inputs.\"\"\"\n",
    "        mask_embedding = self.mask_downscaling(masks)\n",
    "        return mask_embedding\n",
    "\n",
    "    def _get_dive(self) -> torch.device:\n",
    "        return self.point_embeddings[0].weight.device\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        points: Optional[Tuple[torch.Tensor, torch.Tensor]],\n",
    "        boxes: Optional[torch.Tensor],\n",
    "        masks: Optional[torch.Tensor],\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Embeds different types of prompts, returning both sparse and dense\n",
    "        embeddings.\n",
    "\n",
    "        Arguments:\n",
    "          points (tuple(torch.Tensor, torch.Tensor) or none): point coordinates\n",
    "            and labels to embed.\n",
    "          boxes (torch.Tensor or none): boxes to embed\n",
    "          masks (torch.Tensor or none): masks to embed\n",
    "\n",
    "        Returns:\n",
    "          torch.Tensor: sparse embeddings for the points and boxes, with shape\n",
    "            BxNx(embed_dim), where N is determined by the number of input points\n",
    "            and boxes.\n",
    "            : Points, Boxes의 sparse 임베딩\n",
    "          torch.Tensor: dense embeddings for the masks, in the shape\n",
    "            Bx(embed_dim)x(embed_H)x(embed_W)\n",
    "            : Mask 임베딩\n",
    "    \"\"\"\n",
    "    bs = self._get_batch_size(points, boxes, masks)\n",
    "    sparse_embeddings = torch.empty((bs, 0, self.embed_dim), device=self._get_device())\n",
    "\n",
    "    if points is not None:\n",
    "        coords, labels = points\n",
    "        point_embeddings = self._embed_points(coords, labels, pad=(boxes is None))\n",
    "        sparse_embeddings = torch.cat([sparse_embeddings, point_embeddings], dim=1)\n",
    "    if boxes is not None:\n",
    "        box_embeddings = self._embed_boxes(boxes)\n",
    "        sparse_embeddings = torch.cat([sparse_embeddings, box_embeddings], dim=1)\n",
    "    \n",
    "    if masks is not None:\n",
    "        dense_embeddings = self._embed_masks(masks)\n",
    "    else:\n",
    "        dense_embeddings = self.no_mask_embed.weight.reshape(1, -1, 1, 1)\n",
    "            .expand(bs, -1, self.image_embedding_size[0], self.image_embedding_size[1])\n",
    "    return sparse_embeddings, dense_embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mask Decoder\n",
    "- Image embedding과 prompt embedding을 받아 마스크를 예측하는 부분.\n",
    "- Transformer decoder block에 Prompt Self-attention과 Cross-attention을 양방향으로 활용\n",
    "- ![img.jpg](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbFpHTu%2Fbtr8NcL9qZm%2FcxrpfTQaoVn1ytXq4fAPik%2Fimg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 성능\n",
    "- 256개 A100 GPU로 3 ~ 5일 학습\n",
    "- 이미지 인코더는 A100에서 0.15s, 632M 파라미터 수.\n",
    "- 프롬프트 인코더와 마스크 디코더는 CPU에서 0.050s 에 추론 가능하다, 4M 파라미터 수.\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything 기능이란?\n",
    "- <참고 강의>: https://www.youtube.com/watch?v=KQ3haqbIaSk&t=2413s\n",
    "- Everything 기능이란? : 1024개 점(32 x 32)을 64개 씩 16번 나눠서 수행\n",
    "  - 다시말해 특별한 기능이 아니고, SAM을 16 배치로 나눠서 수행\n",
    "- Mask와 iou_prediction 추론 및 필터링\n",
    "  - iou thres 낮은 것 제거\n",
    "  - stability(?) 낮은 것 제거\n",
    "  - nms 겹치는 것 제거\n",
    "- 각 마스크 hole(구멍이 뚤려 있는 마스크는 채워주고)과 island(작게 남아 있는 마스크는 제거)\n",
    "- 강의자 말에 의하면 아래 코드에 Everything 기능 구현이 있다고 한다.\n",
    "  - segment-anything\\segment_anything\\automatic_mask_generator.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "인상적인 후속 주제들\n",
    "- Inpaint Anything\n",
    "- CLIP 과의 결합 (자연어를 해석) \n",
    "  : (개인 의견) 자연어와 결합해서 \n",
    "- Segmentation with tracking\n",
    "  : (개인 의견) 데이터를 수집할때 편할 것 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictor\n",
    "- segment-anything\\segment_anything\\predictor.py\n",
    "- Sam 모델을 사용하기 위한 껍데기라고 보면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "# All rights reserved.\n",
    "\n",
    "# This source code is licensed under the license found in the\n",
    "# LICENSE file in the root directory of this source tree.\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from segment_anything.modeling import Sam\n",
    "\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "from .utils.transforms import ResizeLongestSide\n",
    "\n",
    "class SamPredictor:\n",
    "    def __init__(\n",
    "        self,\n",
    "        sam_model: Sam,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Uses SAM to calculate the image embedding for an image, and then\n",
    "        allow repeated, efficient mask prediction given prompts.\n",
    "\n",
    "        SAM을 사용하여 이미지에 대한 이미지 임베딩을 계산한 다음, 주어진 프롬프르에서\n",
    "        반복적이고 효율적인 마스크 예측 허용\n",
    "\n",
    "        Arguments:\n",
    "          sam_model (Sam): The model to use for mask prediction.\n",
    "                           마스크 예측에 사용할 모델\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.model = sam_model\n",
    "        self.transform = ResizeLongestSide(sam_model.image_encoder.img_size)\n",
    "        self.reset_image()\n",
    "\n",
    "    def set_image(\n",
    "        self,\n",
    "        image: np.ndarray,\n",
    "        image_format: str = \"RGB\",\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Calculates the image embeddings for the provided image, allowing\n",
    "        masks to be predicted with the 'predict' method.\n",
    "\n",
    "        Arguments:\n",
    "          image (np.ndarray): The image for calculating masks. Expects an\n",
    "            image in HWC uint8 format, with pixel values in [0, 255].\n",
    "          image_format (str): The color format of the image, in ['RGB', 'BGR'].\n",
    "        \"\"\"\n",
    "        assert image_format in [\n",
    "            \"RGB\",\n",
    "            \"BGR\",\n",
    "        ], f\"image_format must be in ['RGB', 'BGR'], is {image_format}.\"\n",
    "        if image_format != self.model.image_format:\n",
    "            image = image[..., ::-1]\n",
    "\n",
    "        # Transform the image to the form expected by the model\n",
    "        input_image = self.transform.apply_image(image) # ?\n",
    "        input_image_torch = torch.as_tensor(input_image, device=self.device)\n",
    "        input_image_torch = input_image_torch.permute(2, 0, 1).contiguous()[None, :, :, :]\n",
    "\n",
    "        self.set_torch_image(input_image_torch, image.shape[:2])\n",
    "\n",
    "    @torch.no_grad() # gpu 안쓴다??\n",
    "    def set_torch_image(\n",
    "        self,\n",
    "        transformed_image: torch.Tensor,\n",
    "        original_image_size: Tuple[int, ...],\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Calculates the image embeddings for the provided image, allowing\n",
    "        masks to be predicted with the 'predict' method. Expects the input\n",
    "        image to be already transformed to the format expected by the model.\n",
    "\n",
    "        제공된 이미지의 임베딩을 계산하여, 'predict' 메소드로 마스크를 예측할 수 있다.\n",
    "        입력 이미지가 모델에서 예상하는 형식으로 이미 변환되어 있을 것으로 예상\n",
    "        정의되는 변수\n",
    "        - self.original_size\n",
    "        - self.input_size\n",
    "        - self.features\n",
    "        - self.is_image_set\n",
    "\n",
    "        Arguments:\n",
    "          transformed_image (torch.Tensor): The input image, with shape\n",
    "            1x3xHxW, which has been transformed with ResizeLongestSide.\n",
    "          original_image_size (tuple(int, int)): The size of the image\n",
    "            before transformation, in (H, W) format.\n",
    "\n",
    "            ResizeLongestSide 클래스에 의해 변환된 1x3xHxW 모양 이미지\n",
    "        \"\"\"\n",
    "        assert (\n",
    "            len(transformed_image.shape) == 4 \n",
    "            and transformed_image[1] == 3\n",
    "            and max(*transformed_image.shape[2:]) == self.model.image_encoder.img_size\n",
    "        ), f\"set_torch_image input must be BCHW with long side {self.model.image_encoder.img_size}.\"\n",
    "        self.reset_image()\n",
    "\n",
    "        self.original_size = original_image_size\n",
    "        self.input_size = tuple(transform_image.shape[-2:]) # HW\n",
    "        input_image = self.model.preprocess(transformed_image)\n",
    "        self.features = self.model.image_encoder(input_image) # 임베딩 된것\n",
    "        self.is_image_set = True\n",
    "\n",
    "def predict(\n",
    "        self,\n",
    "        point_coords: Optional[np.ndarray] = None,\n",
    "        point_labels: Optional[np.ndarray] = None,\n",
    "        box: Optional[np.ndarray] = None,\n",
    "        mask_input: Optional[np.ndarray] = None,\n",
    "        multimask_output: bool = True,\n",
    "        return_logits: bool = False,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Predict masks for the given input prompts, using the currently set image.\n",
    "\n",
    "        주어진 입력 프롬프트에서, 마스크를 예측\n",
    "\n",
    "        Arguments:\n",
    "          point_coords (np.ndarray or None): A Nx2 array of point prompts to the\n",
    "            model. Each point is in (X,Y) in pixels.\n",
    "\n",
    "          point_labels (np.ndarray or None): A length N array of labels for the\n",
    "            point prompts. 1 indicates a foreground point and 0 indicates a\n",
    "            background point.\n",
    "\n",
    "            N개 길이 1은 전경, 0은 배경을 가리킨다.\n",
    "\n",
    "          box (np.ndarray or None): A length 4 array given a box prompt to the\n",
    "            model, in XYXY format.\n",
    "\n",
    "            XYXY 포맷으로 주어진 박스\n",
    "\n",
    "          mask_input (np.ndarray): A low resolution mask input to the model, typically\n",
    "            coming from a previous prediction iteration. Has form 1xHxW, where\n",
    "            for SAM, H=W=256.\n",
    "\n",
    "            이전 예측 반복에서 나오는 모델에 대한 저 해상도 마스크 입력. 1 x H x W\n",
    "            SAM의 경우 H=W=256\n",
    "\n",
    "          multimask_output (bool): If true, the model will return three masks.\n",
    "            For ambiguous input prompts (such as a single click), this will often\n",
    "            produce better masks than a single prediction. If only a single\n",
    "            mask is needed, the model's predicted quality score can be used\n",
    "            to select the best mask. For non-ambiguous prompts, such as multiple\n",
    "            input prompts, multimask_output=False can give better results.\n",
    "\n",
    "            true인 경우 3개 마스크를 반환\n",
    "            모호한 입력 프롬프트의 경우(단일 클릭) 더 나은 마스트 생성(?)\n",
    "            단일 마스크만 필요한 경우, 예측 퀄리티를 사용하여 최상의 마스크 선택.\n",
    "            다중 입력 프롬프트 같이 모호하지 않은 경우는, multimask_output=False가 더 나은 결과를 제공할 수 있다.\n",
    "\n",
    "          return_logits (bool): If true, returns un-thresholded masks logits\n",
    "            instead of a binary mask.\n",
    "\n",
    "            true인 경우 바이너리 마스크 대신 마스크 로짓을 반환\n",
    "\n",
    "        Returns:\n",
    "          (np.ndarray): The output masks in CxHxW format, where C is the\n",
    "            number of masks, and (H, W) is the original image size.\n",
    "\n",
    "          (np.ndarray): An array of length C containing the model's\n",
    "            predictions for the quality of each mask.\n",
    "\n",
    "          (np.ndarray): An array of shape CxHxW, where C is the number\n",
    "            of masks and H=W=256. These low resolution logits can be passed to\n",
    "            a subsequent iteration as mask input.\n",
    "\n",
    "          masks: CxHxW 형식의 출력 마스크입니다. 여기서 C는 마스크 수이고 (H, W)는 원본 이미지 크기입니다.\n",
    "\n",
    "          scores: 각 마스크 품질에 대한 모델의 예측을 포함하는 길이 C의 배열\n",
    "          \n",
    "          logits: CxHxW 모양의 배열. C는 마스크 수이고, H=W=256. 저해상도 logits는 마스크 입력으로 후속 반복 작업에 전달될 수 있다.\n",
    "        \"\"\"\n",
    "        if not self.is_image_set:\n",
    "            raise RuntimeError(\"An image must be set with .set_image(...) before mask prediction.\")\n",
    "\n",
    "        # Transform input prompts\n",
    "        coords_torch, labels_torch, box_torch, mask_input_torch = None, None, None, None\n",
    "        \n",
    "        if point_coords is not None:\n",
    "            assert (\n",
    "                point_labels is not None\n",
    "            ), \"point_labels must be supplied if point_coords is supplied.\"\n",
    "\n",
    "            point_coords = self.transform.apply_coords(point_coords, self.original_size) # 포인트를 (이미지를 늘리듯) 타켓 길이에 맞춰 비율 조정해 준다.\n",
    "            coords_torch = torch.as_tensor(point_coords, dtype=torch.float, device=self.device)\n",
    "            labels_torch = torch.as_tensor(point_labels, dtype=torch.int, device=self.device)\n",
    "            coords_torch, labels_torch = coords_torch[None, :, :], labels_torch[None, :]\n",
    "        \n",
    "        if box is not None:\n",
    "            box = self.transform.apply_boxes(box, self.original_size) # 포인트를 (이미지를 늘리듯) 타켓 길이에 맞춰 비율 조정해 준다.\n",
    "            box_torch = torch.as_tensor(box, dtype=torch.float, device=self.device)\n",
    "            box_torch = box_torch[None, :]\n",
    "        \n",
    "        # ★★★ 첫번째 예측할 때는 입력으로 안들어 오고, \n",
    "        # 모호한 입력에 대해 이전 마스크를 입력으로 줄 때 사용된다.\n",
    "        if mask_input is not None: \n",
    "            mask_input_torch = torch.as_tensor(mask_input, dtype=torch.float, device=self.device)\n",
    "            mask_input_torch = mask_input_torch[None, :, :, :]\n",
    "\n",
    "        masks, iou_predictions, low_res_masks = self.predict_torch(\n",
    "            coords_torch,\n",
    "            labels_torch,\n",
    "            box_torch,\n",
    "            mask_input_torch,\n",
    "            multimask_output,\n",
    "            return_logits=return_logits,\n",
    "        )\n",
    "\n",
    "        masks = masks[0].detach().cpu().numpy()\n",
    "        iou_predictions = iou_predictions[0].detach().cpu().numpy()\n",
    "        low_res_masks = low_res_masks[0].detach().cpu().numpy()\n",
    "        return masks, iou_predictions, low_res_masks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_masks(\n",
    "    masks: np.ndarray, iou_preds: np.ndarray, num_points: int\n",
    ") -> Tuple [np.ndarray, np.ndarray]:\n",
    "    # Determine if we should return the multiclick mask or not from the number of points.\n",
    "    # The reweighting is used to avoid control flow.\n",
    "    # Reference: https://github.com/facebookresearch/segment-anything/blob/6fdee8f2727f4506cfbbe553e23b895e27956588/segment_anything/utils/onnx.py#L92-L105\n",
    "    score_reweight = np.array([1000] + [0] * 2)\n",
    "    score = iou_preds + (num_points - 2.5) * score_reweight\n",
    "    best_idx = np.argmax(score)\n",
    "    masks = np.expand_dims(masks[best_idx, :, :], axis=-1)\n",
    "    iou_preds = np.expand_dims(iou_preds[best_idx], axis=0)\n",
    "    return masks, iou_preds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 사용하는 Transformer 블록"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 사용하는 Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    \"\"\"Multi-head Attention block with relative position embeddings.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        num_heads: int = 8,\n",
    "        qkv_bias: bool = True,\n",
    "        use_rel_pos: bool = False,\n",
    "        rel_pos_zero_init: bool = True,\n",
    "        input_size: Optional[Tuple[int, int]] = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = head_dim**-0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim*3, bias=qkv_bias)\n",
    "        self.proj = nn.Linear(dim, dim)   \n",
    "\n",
    "        self.use_rel_pos = use_rel_pos\n",
    "        if self.use_rel_pos:\n",
    "            assert(\n",
    "                input_size is not None\n",
    "            ), \"Input size must be provided if using relative positional encoding.\"\n",
    "            # 상대 포지셔닝 임베딩을 초기화\n",
    "            self.rel_pos_h = nn.Parameter(torch.zeros(2 * input_size[0] - 1, head_dim))\n",
    "            self.rel_pos_w = nn.Parameter(torch.zeros(2 * input_size[1] - 1, head_dim))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B, H, W, _ = x.shape\n",
    "        # qkv with shape (3, B, nHead, H * W, C)\n",
    "        qkv = self.qkv(x).reshape(B, H * W, 3, self.num_heads, -1)\n",
    "        # q, k, v with shape (B * nHead, H * W, C)\n",
    "        q, k, v = qkv.reshape(3, B * self.num_heads, H * W, -1).unbind(0)\n",
    "\n",
    "        attn = (q * self.scale) @ K.transpose(2, -1)\n",
    "\n",
    "        if self.use_rel_pos:\n",
    "            attn = add_decomposed_rel_pos(attn, q, self.rel_pos_h, self.rel_pos_w, (H, W), (H, W))\n",
    "\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        x = (attn @ v).view(B, self.num_heads, H, W, -1).permute(0, 2, 3, 1, 4)\n",
    "        x = x.reshape(B, H, W, -1)\n",
    "        x = self.proj(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rel_pos(q_size: int, k_size: int, rel_pos: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Get relative positional embeddings according to the relative positions of\n",
    "        query and key sizes.\n",
    "    \n",
    "    q, k 사이즈의 상대 포지션에 따른 상대 포지셔닝 임베딩을 구한다. \n",
    "    \n",
    "    Args:\n",
    "        q_size (int): size of query q.\n",
    "        k_size (int): size of key k.\n",
    "        rel_pos (Tensor): relative position embeddings (L, C).\n",
    "\n",
    "    Returns:\n",
    "        Extracted positional embeddings according to relative positions.\n",
    "    \"\"\"\n",
    "    max_rel_dist = int(2 * max(q_size, k_size) - 1) # 최대 상대 거리??\n",
    "    # Interpolate rel pos if needed.\n",
    "    if rel_pos.shape[0] != max_rel_dist:\n",
    "        # Interpolate rel pos.\n",
    "        rel_pos_resized = F.Interpolate(\n",
    "            rel_pos.reshape(1, rel_pos.shape[0], -1).permute(0, 2, 1) # (1, -1, rel_pos.shape[0])\n",
    "            size=max_rel_dist,\n",
    "            mode=\"linear\",\n",
    "        )\n",
    "        rel_pos_resized = rel_pos_resized.reshape(-1, max_rel_dist).permute(1, 0)\n",
    "    else:\n",
    "        rel_pos_resized = rel_pos\n",
    "        \n",
    "    # Scale the coords with short length if shapes for q and k are different.\n",
    "    # Query(Q)의 각 위치에 대한 상대적인 좌표를 계산합니다. q_size보다 k_size가 클 경우, 각 Query 위치를 더 큰 간격으로 매핑\n",
    "    q_coords = torch.arange(q_size)[:, None] * max(k_size / q_size, 1.0) # 열 벡터로 변경\n",
    "    k_coords = torch.arange(k_size)[None, :] * max(q_size / k_size, 1.0) # 핼 벡터로 변경\n",
    "    relative_coords = (q_coords - k_coords) + (k_size - 1) * max(q_size / k_size, 1.0)\n",
    "\n",
    "    # Query와 Key에 대한 상대적인 좌표를 계산하고, 상대적인 위치 정보를 담은 relative_coords라는 텐서를 생성하는 것\n",
    "    return rel_pos_resized[relative_coords.long()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_decomposed_rel_pos(\n",
    "    attn: torch.Tensor,\n",
    "    q: torch.Tensor,\n",
    "    rel_pos_h: torch.Tensor,\n",
    "    rel_pos_w: torch.Tensor,\n",
    "    q_size: Tuple[int, int],\n",
    "    k_size: Tuple[int, int],\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Calculate decomposed Relative Positional Embeddings from :paper:`mvitv2`.\n",
    "    https://github.com/facebookresearch/mvit/blob/19786631e330df9f3622e5402b4a419a263a2c80/mvit/models/attention.py   # noqa B950\n",
    "    Args:\n",
    "        attn (Tensor): attention map.\n",
    "        q (Tensor): query q in the attention layer with shape (B, q_h * q_w, C).\n",
    "        rel_pos_h (Tensor): relative position embeddings (Lh, C) for height axis.\n",
    "        rel_pos_w (Tensor): relative position embeddings (Lw, C) for width axis.\n",
    "        q_size (Tuple): spatial sequence size of query q with (q_h, q_w).\n",
    "        k_size (Tuple): spatial sequence size of key k with (k_h, k_w).\n",
    "\n",
    "    Returns:\n",
    "        attn (Tensor): attention map with added relative positional embeddings.\n",
    "    \"\"\"\n",
    "    q_h, q_w = q_size\n",
    "    k_h, k_w = k_size\n",
    "    Rh = get_rel_pos(q_h, k_h, rel_pos_h)\n",
    "    Rw = get_rel_pos(q_w, k_w, rel_pos_w)\n",
    "\n",
    "    B, _, dim = q.shape\n",
    "    r_q = q.reshape(B, q_h, q_w, dim) # (B, H, W, C)\n",
    "    rel_h = torch.einsum(\"bhwc,hkc->bhwk\", r_q, Rh)\n",
    "    rel_w = torch.einsum(\"bhwc,wkc->bhwk\", r_q, Rw)\n",
    "\n",
    "    # rel_h[:, :, :, :, None]과 rel_w[:, :, :, None, :]는 각각 Query와 Key에 대한 상대적인 위치를 나타내는 텐서들\n",
    "    # 이 텐서들은 브로드캐스팅을 활용하여 attn 텐서의 각 위치에 대해 상대적인 위치 정보를 더합니다\n",
    "    # \n",
    "    attn = (\n",
    "        attn.view(B, q_h, q_w, k_h, k_w) + rel_h[:, :, :, :, None] + rel_w[:, :, :, None, :]\n",
    "    ).view(B, q_h * q_w, k_h * k_w)\n",
    "\n",
    "    # (B, q_h * q_w, k_h * k_w) 모양으로 변경합니다. 이는 어텐션 스코어 행렬을 표현하는 것으로, 각 Query와 Key의 조합에 대한 유사도를 나타낸다\n",
    "    # 이 코드는 주어진 어텐션 스코어 행렬 attn에 상대적인 위치 정보를 더하여 최종 어텐션 스코어 행렬을 재구성하는 작업을 수행\n",
    "    return attn   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.17 ('mm3d')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d69b847d396095f32f676329057412de0235e6809d7fbe768ce2e81f9c069d24"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
