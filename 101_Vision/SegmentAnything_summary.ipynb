{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<목차>\n",
    "- Image Segmetation을 위한 Foundation Model 모델 설계를 위한 Task 정의\n",
    "- 데이터 수집 과정\n",
    "  - Assisted-manual\n",
    "  - Semi-automatic\n",
    "  - Fully-automatic\n",
    "- 모델 구조\n",
    "  - Image Encoder\n",
    "  - Prompt Encoder\n",
    "  - Mask Decoder\n",
    "  - 성능\n",
    "- Everything 기능이란?\n",
    "- 인상적인 후속 주제들\n",
    "- Predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Segmetation 기능을 하는 Foundation Model 모델 설계를 위한 Task 정의\n",
    "이미지 Segmentation 모델(SAM)을 만들기 위한 Foundation Model을 설계하기 위해서 한 3가지 질문\n",
    "1. What task will enable zero-shot generalization?  \n",
    "2. What is the corresponding model architecture?\n",
    "3. What data can power this task and model? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (처음 본 사물도 인식할 수 있는) zero-shot 일반화를 위한 Task는 어떤 Task인가?  \n",
    "  --> `어떻하면 애매모호한 상황도 모두 대처할 수 있는 모델을 만들 수 있을까? Promptable Segmentation Task를 정의`.  \n",
    "      점(point), 박스(box), 마스크(mask), 텍스트(text)를 입력으로 받을 수 있게 설계.  \n",
    "  --> `모호한 Prompt가 주어졌을 때도 합리적인 mask를 출력해야 한다. 모호하다면 연관된 것을 다 확률로 나타낸다`.\n",
    "  \n",
    "- 이에 상응하는 모델 구조는 어떻게 되야 하는가?  \n",
    "  --> Promt를 당연히 지원해야 한다.\n",
    "  \n",
    "- 어떤 데이터가 필요한가?  \n",
    "  --> Promptable Segmentation Task를 위한 Segmentation Mask는 구하기 어렵다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 수집 과정\n",
    "1. `Assisted-manual`\n",
    "   - 공개된 Segmentation Dataset을 이용해 SAM 초기 학습\n",
    "   - 웹 기반 인터페이스에서 초기 학습된 SAM을 이용해 데이터 생성\n",
    "   - 새로 취득한 Data로 점진적 모델 학습(6회)\n",
    "   - 120k 이미지로 부터 4.3M Mask 취득  \n",
    "2. `Semi-automatic`\n",
    "   - Mask의 종류를 다양화 하는 것을 목표로 함\n",
    "   - 1단계에서 학습된 신뢰도 높은 Mask를 작업 화면에 표시\n",
    "   - Annotator들은 그 외 Object를 작업\n",
    "   - 새로 취득한 Data로 점진적 모델 학습(5회)\n",
    "   - 180k 이미지로 부터 5.9M Mask 취득  \n",
    "3. `Fully automatic`\n",
    "   - 완전 자동화된 Annotation 단계\n",
    "   - 이미지에 32 x 32 Regular Grid Point를 각 포인트 마다 대응되는 Mask가 할당된다.\n",
    "   - 그 중에서 IoU가 높은 Mask만 남긴다.\n",
    "   - 중복된 Mask 제거 등 후처리 작업 진행\n",
    "   - 1.1M 이미지로 부터 1.1B Mask 취득"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 구조\n",
    "\n",
    "![image.png](https://github.com/facebookresearch/segment-anything/raw/main/assets/model_diagram.png?raw=true)\n",
    "- Image encoder output인 image embedding, Prompt encoder output인 prompt embedding을 `두개의 embedding을 Lightweight Mask decoder에서 결합하여 Mask 예측`\n",
    "- Prompt encoder + Mask decoder는 50ms 이내로 수행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Powerful Image Encoder\n",
    "- Prompt Encoder: Sparse/Dense Prompt로 나뉜다.\n",
    "  - Sparse Prompt : Points, Boxes, Text를 임베딩 한다.\n",
    "  - Dense Prompt : Mask을 임베딩 한다.\n",
    "- Mask Decoder\n",
    "\n",
    "※ 참고로 오픈소스에서는 Text를 임베딩하는 기능은 없다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prompt Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (267457045.py, line 11)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 11\u001b[1;36m\u001b[0m\n\u001b[1;33m    super().__init__()\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "# segment-anything\\segment_anything\\modeling\\prompt_encoder.py\n",
    "class PromptEncoder(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim: int,\n",
    "        image_embedding_size: Tuple[int, int],\n",
    "        input_image_size: Tuple[int, int],\n",
    "        mask_in_chans: int,\n",
    "        activation: Type[nn.Module] = nn.GELU,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "    \n",
    "        self.mask_downscaling = nn.Sequential(\n",
    "            nn.Conv2d(1, mask_in_chans // 4, kernel_size=2, stride=2),\n",
    "            LayerNorm2d(mask_in_chans//4),\n",
    "            activation(),\n",
    "            nn.Conv2d(mask_in_chans // 4, mask_in_chans, kernel_size=2, stride=2),\n",
    "            LayerNorm2d(mask_in_chans),\n",
    "            activation()\n",
    "            nn.Conv2d(mask_in_chans, embed_dim, kernel_size=1),\n",
    "        )\n",
    "\n",
    "    def _get_batch_size(\n",
    "        self,\n",
    "        points: Optional[Tuple[torch.Tensor, torch.Tensor]],\n",
    "        boxes: Optional[torch.Tensor],\n",
    "        masks: Optional[torch.Tensor],\n",
    "    ) -> int:\n",
    "        \"\"\"Gets the batch size of the output given the batch size of the input\n",
    "        prompts.\"\"\"\n",
    "        if points is not None:\n",
    "            return points[0].shape[0]\n",
    "        elif boxes is not None:\n",
    "            return boxes.shape[0]\n",
    "        elif masks is not None:\n",
    "            return masks.shape[0]\n",
    "        else:\n",
    "            return 1\n",
    "\n",
    "\n",
    "\n",
    "    def _embed_masks(self, masks: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Embeds mask inputs.\"\"\"\n",
    "        mask_embedding = self.mask_downscaling(masks)\n",
    "        return mask_embedding\n",
    "\n",
    "    def _get_dive(self) -> torch.device:\n",
    "        return self.point_embeddings[0].weight.device\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        points: Optional[Tuple[torch.Tensor, torch.Tensor]],\n",
    "        boxes: Optional[torch.Tensor],\n",
    "        masks: Optional[torch.Tensor],\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Embeds different types of prompts, returning both sparse and dense\n",
    "        embeddings.\n",
    "\n",
    "        Arguments:\n",
    "          points (tuple(torch.Tensor, torch.Tensor) or none): point coordinates\n",
    "            and labels to embed.\n",
    "          boxes (torch.Tensor or none): boxes to embed\n",
    "          masks (torch.Tensor or none): masks to embed\n",
    "\n",
    "        Returns:\n",
    "          torch.Tensor: sparse embeddings for the points and boxes, with shape\n",
    "            BxNx(embed_dim), where N is determined by the number of input points\n",
    "            and boxes.\n",
    "            : Points, Boxes의 sparse 임베딩\n",
    "          torch.Tensor: dense embeddings for the masks, in the shape\n",
    "            Bx(embed_dim)x(embed_H)x(embed_W)\n",
    "            : Mask 임베딩\n",
    "    \"\"\"\n",
    "    bs = self._get_batch_size(points, boxes, masks)\n",
    "    sparse_embeddings = torch.empty((bs, 0, self.embed_dim), device=self._get_device())\n",
    "\n",
    "    if points is not None:\n",
    "        coords, labels = points\n",
    "        point_embeddings = self._embed_points(coords, labels, pad=(boxes is None))\n",
    "        sparse_embeddings = torch.cat([sparse_embeddings, point_embeddings], dim=1)\n",
    "    if boxes is not None:\n",
    "        box_embeddings = self._embed_boxes(boxes)\n",
    "        sparse_embeddings = torch.cat([sparse_embeddings, box_embeddings], dim=1)\n",
    "    \n",
    "    if masks is not None:\n",
    "        dense_embeddings = self._embed_masks(masks)\n",
    "    else:\n",
    "        dense_embeddings = self.no_mask_embed.weight.reshape(1, -1, 1, 1)\n",
    "            .expand(bs, -1, self.image_embedding_size[0], self.image_embedding_size[1])\n",
    "    return sparse_embeddings, dense_embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mask Decoder\n",
    "- Image embedding과 prompt embedding을 받아 마스크를 예측하는 부분.\n",
    "- Transformer decoder block에 Prompt Self-attention과 Cross-attention을 양방향으로 활용\n",
    "- ![img.jpg](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbFpHTu%2Fbtr8NcL9qZm%2FcxrpfTQaoVn1ytXq4fAPik%2Fimg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 성능\n",
    "- 256개 A100 GPU로 3 ~ 5일 학습\n",
    "- 이미지 인코더는 A100에서 0.15s, 632M 파라미터 수.\n",
    "- 프롬프트 인코더와 마스크 디코더는 CPU에서 0.050s 에 추론 가능하다, 4M 파라미터 수.\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything 기능이란?\n",
    "- <참고 강의>: https://www.youtube.com/watch?v=KQ3haqbIaSk&t=2413s\n",
    "- Everything 기능이란? : 1024개 점(32 x 32)을 64개 씩 16번 나눠서 수행\n",
    "  - 다시말해 특별한 기능이 아니고, SAM을 16 배치로 나눠서 수행\n",
    "- Mask와 iou_prediction 추론 및 필터링\n",
    "  - iou thres 낮은 것 제거\n",
    "  - stability(?) 낮은 것 제거\n",
    "  - nms 겹치는 것 제거\n",
    "- 각 마스크 hole(구멍이 뚤려 있는 마스크는 채워주고)과 island(작게 남아 있는 마스크는 제거)\n",
    "- 강의자 말에 의하면 아래 코드에 Everything 기능 구현이 있다고 한다.\n",
    "  - segment-anything\\segment_anything\\automatic_mask_generator.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "인상적인 후속 주제들\n",
    "- Inpaint Anything\n",
    "- CLIP 과의 결합 (자연어를 해석) \n",
    "  : (개인 의견) 자연어와 결합해서 \n",
    "- Segmentation with tracking\n",
    "  : (개인 의견) 데이터를 수집할때 편할 것 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictor\n",
    "- segment-anything\\segment_anything\\predictor.py\n",
    "- Sam 모델을 사용하기 위한 껍데기라고 보면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SamPredictor:\n",
    "    def __init__(\n",
    "        self,\n",
    "        sam_model: Sam,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Uses SAM to calculate the image embedding for an image, and then\n",
    "        allow repeated, efficient mask prediction given prompts.\n",
    "\n",
    "        Arguments:\n",
    "          sam_model (Sam): The model to use for mask prediction.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.model = sam_model\n",
    "        self.transform = ResizeLongestSide(sam_model.image_encoder.img_size)\n",
    "        self.reset_image()\n",
    "\n",
    "    def set_image(\n",
    "        self,\n",
    "        image: np.ndarray,\n",
    "        image_format: str = \"RGB\",\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Calculates the image embeddings for the provided image, allowing\n",
    "        masks to be predicted with the 'predict' method.\n",
    "\n",
    "        Arguments:\n",
    "          image (np.ndarray): The image for calculating masks. Expects an\n",
    "            image in HWC uint8 format, with pixel values in [0, 255].\n",
    "          image_format (str): The color format of the image, in ['RGB', 'BGR'].\n",
    "        \"\"\"\n",
    "        assert image_format in [\n",
    "            \"RGB\",\n",
    "            \"BGR\",\n",
    "        ], f\"image_format must be in ['RGB', 'BGR'], is {image_format}.\"\n",
    "        if image_format != self.model.image_format:\n",
    "            image = image[..., ::-1]\n",
    "\n",
    "        # Transform the image to the form expected by the model\n",
    "        input_image = self.transform.apply_image(image) # ?\n",
    "        input_image_torch = torch.as_tensor(input_image, device=self.device)\n",
    "        input_image_torch = input_image_torch.permute(2, 0, 1).contiguous()[None, :, :, :]\n",
    "\n",
    "        self.set_torch_image(input_image_torch, image.shape[:2])\n",
    "\n",
    "    @torch.no_grad() # gpu 안쓴다??\n",
    "    def set_torch_image(\n",
    "        self,\n",
    "        transformed_image: torch.Tensor,\n",
    "        original_image_size: Tuple[int, ...],\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        이미지 임베딩을 계산. 'predict' 메소드로 마스크를 예측하게 한다. \n",
    "\n",
    "        Arguments:\n",
    "          transformed_image (torch.Tensor): ResizeLongestSide 클래스에 의해 변환된 1x3xHxW 모양 이미지\n",
    "          original_image_size (tuple(int, int)): 변환전 이미지 사이즈\n",
    "        \"\"\"\n",
    "        assert (\n",
    "            len(transformed_image.shape) == 4 \n",
    "            and transformed_image[1] == 3\n",
    "            and max(*transformed_image.shape[2:]) == self.model.image_encoder.img_size\n",
    "        ), f\"set_torch_image input must be BCHW with long side {self.model.image_encoder.img_size}.\"\n",
    "        self.reset_image()\n",
    "\n",
    "        self.original_size = original_image_size\n",
    "        self.input_size = tuple(transform_image.shape[-2:]) # HW\n",
    "        input_image = self.model.preprocess(transformed_image)\n",
    "        self.features = self.model.image_encoder(input_image) # 임베딩 된것\n",
    "        self.is_image_set = True\n",
    "\n",
    "    def predict(\n",
    "        self,\n",
    "        point_coords: Optional[np.ndarray] = None,\n",
    "        point_labels: Optional[np.ndarray] = None,\n",
    "        box: Optional[np.ndarray] = None,\n",
    "        mask_input: Optional[np.ndarray] = None,\n",
    "        multimask_output: bool = True,\n",
    "        return_logits: bool = False,\n",
    "    ) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:    \n",
    "        \"\"\"\n",
    "\n",
    "        Arguments:\n",
    "          point_coords (np.ndarray or None): N x 2 크기의 배열. N개 점의 (X, Y) 위치. 사용자가 점을 찍는 위치들\n",
    "          point_labels (np.ndarray or None): point prompts를 위한 N 배열의 라벨 1은 전면, 0은 배경 을 나타낸다.\n",
    "                                             사물이나 배경이나 지정을 해줘야 되는 구나..\n",
    "\n",
    "\n",
    "        \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_masks(\n",
    "    masks: np.ndarray, iou_preds: np.ndarray, num_points: int\n",
    ") -> Tuple [np.ndarray, np.ndarray]:\n",
    "    # Determine if we should return the multiclick mask or not from the number of points.\n",
    "    # The reweighting is used to avoid control flow.\n",
    "    # Reference: https://github.com/facebookresearch/segment-anything/blob/6fdee8f2727f4506cfbbe553e23b895e27956588/segment_anything/utils/onnx.py#L92-L105\n",
    "    score_reweight = np.array([1000] + [0] * 2)\n",
    "    score = iou_preds + (num_points - 2.5) * score_reweight\n",
    "    best_idx = np.argmax(score)\n",
    "    masks = np.expand_dims(masks[best_idx, :, :], axis=-1)\n",
    "    iou_preds = np.expand_dims(iou_preds[best_idx], axis=0)\n",
    "    return masks, iou_preds\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.17 ('mm3d')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d69b847d396095f32f676329057412de0235e6809d7fbe768ce2e81f9c069d24"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
